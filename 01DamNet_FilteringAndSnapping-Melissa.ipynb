{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10745787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written 8/11/2022 (updated 6/27/2024) to automate the filtering process and assign dam order to all dams in the nation.\n",
    "# The starting file for this script is a csv file of the NID database dams for the entire nation \n",
    "# downloaded from https://nid.sec.usace.army.mil/#/downloads. You should be able to run this for only a subset\n",
    "# of the dams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce31f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with Python 3 and ArcGIS Pro v 3.2.2. User needs and ArcGIS Pro installation to use the arcpy package in this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efda67",
   "metadata": {},
   "source": [
    "# 1. Import databases and filter them. Combine databases as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876b39f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from simpledbf import Dbf5\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Import arcpy packages\n",
    "import archook\n",
    "arcgis_python_path = r'C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3' #The path on your computer where your ArcGIS Python installation is located\n",
    "\n",
    "# Add the Python environment to the path\n",
    "archook.arcpy = arcgis_python_path\n",
    "\n",
    "# Locate arcgis and access arcpy\n",
    "# # archook.get_arcpy(pro=True) # pro=True argument may not be needed depending on archook version. If so, use:\n",
    "# archook.get_arcpy() \n",
    "\n",
    "import arcpy\n",
    "\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bffc-8318-4308-bf93-bd9d14cd4472",
   "metadata": {},
   "source": [
    "*You may need to clone your arcgis environment and run from this activated environment for the line <archook.get_arcpy(pro=True)> to run. Otherwise you may receive an ImportError. More info on archook found here: https://pypi.org/project/archook-dbc/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae555cdc",
   "metadata": {},
   "source": [
    "#### Load data here. You will need to replace all of the input files with the locations of your copies of the file. The output_folder variable is the folder where all of the output files will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6357611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\808854380.py:15: DtypeWarning: Columns (53) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  GDAT = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GDAT_v1_dams.csv')\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\808854380.py:28: DtypeWarning: Columns (12,15,16,30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  damnet = pd.read_csv(r\"E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\Outputs_Melissa\\NIDsMappedInBasins_MedRes_102524.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of NID in database: (91886, 84)\n",
      "Number of USBR in database: (257, 34)\n",
      "Number of USACE in database: (465, 34)\n",
      "Number of sites in database: (1066, 34)\n"
     ]
    }
   ],
   "source": [
    "## Load data. See the files included with the report for formatting. Also load in the raw files (not the cross-ref files) and cross-ref\n",
    "## in the code.\n",
    "\n",
    "# Load NID data (NID downloaded 06/19/2024)\n",
    "NIDs = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID2024.csv', header=1, low_memory=False) \n",
    "\n",
    "# Load removed dams file, if using\n",
    "removed = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/RemovedDams.csv')\n",
    "\n",
    "# Load GeoDAR data to use as location where no NID location (GeoDAR v11)\n",
    "geoDAR = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GeoDAR_v11_dams.csv')\n",
    "# geoDAR = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GeoDAR_crossref.csv')\n",
    "\n",
    "#Load GDAT data to use for additional storage and year complete (GDAT v1)\n",
    "GDAT = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GDAT_v1_dams.csv')\n",
    "\n",
    "# Load the dam attribute and location file for if you have sites with additional data or better locations that you want\n",
    "# included. This can include completion year or storage data. This is where you tag dams for the SiteTag function.\n",
    "sites = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/attributes_Mel.csv') \n",
    "\n",
    "# Load NHD Plus Medium Resolution flowline shapefile (NHDPlus v2)\n",
    "NHDFlowline = 'E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NHDFlowline_Network_NHDPlus_Countries.gdb/NHDFlowline_Network_NHDPlus_Countries'  # Path for NHD flowline shapefile\n",
    "\n",
    "# Load GRanD data. (GRanD v1.3 with modifications to locations that places GRanD on NHD Flowlines)\n",
    "GRanD = pd.read_csv('E:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GRanD_dams_v1_3.csv')\n",
    "\n",
    "# Load the most recent version of DamNet to transfer ShortIDs for the new run for consistency.\n",
    "damnet = pd.read_csv(r\"E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\Outputs_Melissa\\NIDsMappedInBasins_MedRes_102524.csv\")\n",
    "\n",
    "# Load the dataset cross-reference file.\n",
    "crossref = pd.read_csv(r'E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\FilesToUpload\\damnet_datacrossreference.csv')\n",
    "\n",
    "# Assign output folder directory where you want to save the output files.\n",
    "out_folder = 'E:/ResSed/MediumResolution_DamLinkages/Manuscript/Outputs_Nov7' # Write full path to this folder for arcgis outputs to be saved properly\n",
    "\n",
    "# Print original numbers of dams in various databases for tracking:\n",
    "print('Original number of NID in database:',NIDs.shape)\n",
    "print('Number of USBR in database:',sites.loc[sites.IsUSBR==1].shape)\n",
    "print('Number of USACE in database:',sites.loc[sites.IsUSACE==1].shape)\n",
    "print('Number of sites in database:',sites.loc[sites.IsSite==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "773e114b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw NIDs with no locations: (2, 84)\n"
     ]
    }
   ],
   "source": [
    "# Count how many dams have lat/long as 0\n",
    "noLoc = NIDs[(NIDs.Latitude == 0) | (NIDs.Longitude == 0)]\n",
    "\n",
    "print('Number of raw NIDs with no locations:', noLoc.shape)\n",
    "#noLoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16571b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NID data to only keep the columns that we need\n",
    "# FOR 2024 NID FILE: Column headers may change between versions of NID downloads. Check this if re-running with a new NID download\n",
    "\n",
    "# Rename columns\n",
    "new_cols = ['Dam_Name','Other_Dam','Former_Name','NID','OtherStructureID','FederalID','Owner_Name','OwnerTypes','PrimaryOwnerType','NumStruct','AssStruct','Designer','NonFedDam','PrimaryPurp', 'Purp','SourceAgency','StateorFedID','Latitude','Longitude','State','County','City','DisttoCity','River','CongressDist','AmInd','SecLoc','StateReg','Juris','Agency','StatePerm','StateInsp','StateEnforce','FedReg','FedOwner','FedFunding','FedDesign','FedConst','FedReg','FedInsp','FedOps','FedOther','SecAg','NRCS','PrimDamType','DamTypes','CoreTypes','Foundation','Dam_Height','HydraulicHeight','StructHeight','NID_Height','NIDHeightCat','Dam_Length','Volume','Year_Compl','YearCompCat','Year_Modif','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','SpillwayType','SpillWidth','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','OutletGate','DataUpdated','LastInspection','InspectionFreq','HazardClass','CondAss','CondAssDate','OpStat','OpStatDate','EAPPrep','EAPRev','InundationMap','URL']\n",
    "NIDs.columns = new_cols\n",
    "\n",
    "\n",
    "# Filter and rename variables\n",
    "NIDs = NIDs[['Dam_Name','Other_Dam','NID','OtherStructureID','FederalID','Longitude','Latitude','State','River','Owner_Name','OwnerTypes','Year_Compl','Year_Modif','NID_Height','Dam_Length','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','PrimaryPurp','Purp','PrimDamType','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth']]\n",
    "#Strip white spaces from NID IDs\n",
    "NIDs['NID'] = NIDs['NID'].str.replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a41717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign Short ID to NID\n",
    "NIDs = pd.merge(NIDs, damnet[['NID','ShortID']],how = 'left', on = 'NID')\n",
    "\n",
    "# Merge input datasets with cross-reference file to transfer NID\n",
    "geoDAR = pd.merge(geoDAR,crossref[['NID','GeoDAR_id_v11']],how = 'left',left_on='id_v11',right_on = 'GeoDAR_id_v11')\n",
    "GDAT = pd.merge(GDAT,crossref[['NID','GDAT_Feature_ID']],how = 'left', left_on = 'Feature_ID', right_on = 'GDAT_Feature_ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb5854",
   "metadata": {},
   "source": [
    "#### Move and delete NID dams using the spatial edits file if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26833d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length to delete: (211, 34)\n",
      "NID database size: (91861, 63)\n",
      "Number of Moved dams in database: (2591, 63)\n",
      "Number of sites: (1228, 63)\n"
     ]
    }
   ],
   "source": [
    "# Use spatial edit file to change lat/long of moved dams and remove deleted dams. If you do not wish to modify original NID, skip this.\n",
    "delete = sites.loc[sites.Deleted == 1]\n",
    "move = sites.loc[sites.Moved == 1]\n",
    "\n",
    "NIDs = NIDs[~NIDs['NID'].isin(delete['NID'])] # Drop deleted NIDs\n",
    "\n",
    "NID_join = pd.merge(NIDs, move, on='NID', how = 'outer') # Add moved NIDs to NID dataframe\n",
    "\n",
    "\n",
    "# Change the Latitude and Longitude fields of the NIDs you wish to move\n",
    "NID_join.loc[NID_join.Moved == 1,'Latitude'] = NID_join.Lat \n",
    "NID_join.loc[NID_join.Moved == 1, 'Longitude'] = NID_join.Long \n",
    "\n",
    "NID_join.Moved = NID_join.Moved.fillna(0)\n",
    "\n",
    "print('Length to delete:', delete.shape)\n",
    "print('NID database size:',NID_join.shape) # New database size after incorporating dam attribute file\n",
    "print('Number of Moved dams in database:', NID_join.loc[NID_join.Moved == 1].shape) \n",
    "print('Number of sites:',NID_join.loc[NID_join.IsSite==1].shape)\n",
    "\n",
    "# Combine duplicate ShortID fields from join\n",
    "NID_join['ShortID'] = NID_join.ShortID_x\n",
    "NID_join.loc[NID_join['ShortID'].isna(),'ShortID'] = NID_join.ShortID_y\n",
    "\n",
    "NID_join = NID_join.drop(columns = ['ShortID_x','ShortID_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b3f803d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaNs with 0\n",
    "NID_join.NID_Storag = NID_join.NID_Storag.fillna(0)\n",
    "NID_join.Max_Storag = NID_join.Max_Storag.fillna(0)\n",
    "NID_join.Normal_Sto = NID_join.Normal_Sto.fillna(0)\n",
    "NID_join.IsUSBR = NID_join.IsUSBR.fillna(0)\n",
    "NID_join.IsUSACE = NID_join.IsUSACE.fillna(0)\n",
    "NID_join.IsSite = NID_join.IsSite.fillna(0)\n",
    "NID_join.IsRiverMth = NID_join.IsRiverMth.fillna(0)\n",
    "NID_join.Moved = NID_join.Moved.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c91a",
   "metadata": {},
   "source": [
    "#### Remove duplicate NIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27969ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing duplicates and joining to site file: (91033, 63)\n",
      "Number of sites in database: (1066, 63)\n",
      "Number of Reclamation dams in database: (257, 63)\n",
      "Number of USACE dams in database: (465, 63)\n",
      "Number of Moved dams in database: (2229, 63)\n",
      "Number of rivers in database: (143, 63)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate NIDs. Keep dam with largest reported storage data.\n",
    "NID_join = NID_join.drop(index=NID_join.loc[NID_join.OtherStructureID.notnull()].index)\n",
    "\n",
    "# A couple of dams don't have Other Structure ID but are duplicates. Filter those by storage. Should only be 2\n",
    "# Sort the data by descending max storage\n",
    "NID_join = NID_join.sort_values('NID_Storag', ascending = False)\n",
    "\n",
    "# Remove duplicate NIDs, keeping the first value aka the biggest capacity\n",
    "bool_series = NID_join['NID'].duplicated()\n",
    "NID_join = NID_join[~bool_series]\n",
    "\n",
    "NID_join = NID_join.reset_index()\n",
    "\n",
    "# Print checks\n",
    "print('Size after removing duplicates and joining to site file:',NID_join.shape) # New database size after removing duplicate NIDs\n",
    "print('Number of sites in database:',NID_join.loc[(NID_join.IsSite == 1)].shape)\n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape) \n",
    "print('Number of Moved dams in database:', NID_join.loc[NID_join.Moved == 1].shape)\n",
    "print('Number of rivers in database:', NID_join.loc[NID_join.IsRiverMth==1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991791f-db68-414d-935d-c80df0d41334",
   "metadata": {},
   "source": [
    "#### Locate and populate lock dams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb4f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate IsLock column\n",
    "# Replace null values with 0\n",
    "# Assign 1 to any NID field with lock information\n",
    "NID_join.NumLocks = NID_join.NumLocks.fillna(0)\n",
    "NID_join.LengthLocks = NID_join.LengthLocks.fillna(0)\n",
    "NID_join.LockWidth = NID_join.LockWidth.fillna(0)\n",
    "NID_join.LengthSecondLock = NID_join.LengthSecondLock.fillna(0)\n",
    "NID_join.SecondLockWidth = NID_join.SecondLockWidth.fillna(0)\n",
    "NID_join.loc[(NID_join.NumLocks>0) & (NID_join.NumLocks<10),'IsLock'] = 1 \n",
    "NID_join.loc[(NID_join.LengthLocks>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LockWidth>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LengthSecondLock>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.SecondLockWidth>0),'IsLock'] = 1\n",
    "\n",
    "# Search by name containing word 'Lock'\n",
    "NID_join['Dam_Name'] = NID_join['Dam_Name'].fillna('') # First fill NA names with empty strings\n",
    "NID_join.loc[NID_join['Dam_Name'].str.contains('Lock '), 'IsLock'] = 1 \n",
    "\n",
    "# Set GA01804 and MO20537 to IsLock = 0 because are not locks but have lock in name\n",
    "NID_join.loc[NID_join['NID'] == 'GA01804', 'IsLock'] = 0\n",
    "NID_join.loc[NID_join['NID'] == 'MO20537', 'IsLock'] = 0\n",
    "\n",
    "# Fill null values with 0\n",
    "NID_join.IsLock = NID_join.IsLock.fillna(0)\n",
    "\n",
    "# Drop columns we no longer need.\n",
    "NID_join = NID_join.drop(['NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','Deleted'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e060",
   "metadata": {},
   "source": [
    "#### Filter dams by name to remove any dam names that contain Spillway, Levee, Sewage, Treatment, Auxiliary, or Remedial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a47a511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before filtering: (91033, 57)\n",
      "Size after filtering by name: (90563, 57)\n",
      "Number of sites in database: (1066, 57)\n",
      "Number of Reclamation dams in database: (257, 57)\n",
      "Number of USACE dams in database: (465, 57)\n"
     ]
    }
   ],
   "source": [
    "print('Size before filtering:', NID_join.shape)\n",
    "\n",
    "# Filter by name: Filter the dams that are NOT moved in the dam attributes file\n",
    "filters = \"Spillway|Levee|Sewage|Treatment|Auxiliary|Remedial\"\n",
    "\n",
    "# Only filter non-sites\n",
    "NIDs_filtered = NID_join.drop(index=NID_join.loc[NID_join.Dam_Name.str.contains(filters)==True].loc[(NID_join.Moved == 0)].index)\n",
    "\n",
    "NID_join = NIDs_filtered\n",
    "\n",
    "print('Size after filtering by name:', NIDs_filtered.shape) # New database size after filtering by name\n",
    "print('Number of sites in database:', NID_join.loc[NID_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7e560",
   "metadata": {},
   "source": [
    "#### Join remaining supplementary files and modify latitude and longitude appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18036348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of GRanD: (1901, 23)\n"
     ]
    }
   ],
   "source": [
    "## Join to GRanD dams\n",
    "\n",
    "# Filter so only dams in the United States\n",
    "GRanD = GRanD[(GRanD['COUNTRY'].str.contains('United States')== True)|(GRanD['SEC_CNTRY'].str.contains('United States')==True)]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Alaska') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Hawaii') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Puerto Rico') == False]\n",
    "                \n",
    "# Filter out columns we don't want\n",
    "GRanD = GRanD.drop(columns = ['RIVER','ALT_RIVER','MAIN_BASIN','SUB_BASIN','NEAR_CITY','ALT_CITY','SEC_ADMIN','COUNTRY','SEC_CNTRY','ALT_YEAR','ALT_HGT_M','DAM_LEN_M','ALT_LEN_M','AREA_SKM','AREA_POLY','AREA_REP','AREA_MAX','AREA_MIN','CAP_MAX','CAP_REP','CAP_MIN','DEPTH_M','DIS_AVG_LS','DOR_PC','ELEV_MASL','CATCH_SKM','CATCH_REP','DATA_INFO','USE_IRRI','USE_ELEC','USE_SUPP','USE_FCON','USE_RECR','USE_NAVI','USE_FISH','USE_PCON','USE_LIVE','USE_OTHR','MAIN_USE','LAKE_CTRL','MULTI_DAMS','TIMELINE','COMMENTS','URL','QUALITY','EDITOR','POLY_SRC'])\n",
    "\n",
    "print('Size of GRanD:', GRanD.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "870da7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before joining to GRanD: (90563, 57)\n",
      "Size after joining to GRanD: (90625, 87)\n",
      "Number of sites in database: (1066, 87)\n",
      "Number of Reclamation dams in database: (257, 87)\n",
      "Number of USACE dams in database: (465, 87)\n",
      "Number of GRanD in database: (1898, 87)\n"
     ]
    }
   ],
   "source": [
    "# Find matches between GRanD NID and NID NID; outer join should keep GRanD dams that weren't in NID\n",
    "GRanD_join = pd.merge(NID_join, GRanD, on='NID', how='outer', suffixes = ('_NID','_GRanD'))\n",
    "\n",
    "# Combine output columns that were split in the join\n",
    "GRanD_join.loc[GRanD_join.ShortID_GRanD.notnull(), 'ShortID'] = GRanD_join.ShortID_GRanD\n",
    "GRanD_join.loc[GRanD_join.ShortID.isnull(), 'ShortID'] = GRanD_join.ShortID_NID\n",
    "\n",
    "# Assign 0 to NaNs\n",
    "GRanD_join.loc[GRanD_join.IsSite_NID.isna(),'IsSite_NID'] = 0\n",
    "GRanD_join['IsSite'] = GRanD_join['IsSite_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSBR_NID.isna(),'IsUSBR_NID'] = 0\n",
    "GRanD_join['IsUSBR'] = GRanD_join['IsUSBR_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSACE_NID.isna(),'IsUSACE_NID'] = 0\n",
    "GRanD_join['IsUSACE'] = GRanD_join['IsUSACE_NID']\n",
    "\n",
    "# Create a GRanD lat/long field that takes the GRanD lat/long preferentially. These fields are called NewX and NewY for our manual placements.\n",
    "GRanD_join['LAT_GRAND'] = GRanD_join['NewY']\n",
    "GRanD_join['LONG_GRAND'] = GRanD_join['NewX']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_GRAND = GRanD_join.LAT_GRAND.fillna(-999)\n",
    "GRanD_join.LONG_GRAND = GRanD_join.LONG_GRAND.fillna(-999)\n",
    "\n",
    "# Create a new field for lat/long that takes the GRanD lat/long if not -999 and takes the original lat/long if -999\n",
    "GRanD_join['LAT_JOIN'] = GRanD_join.LAT_GRAND\n",
    "GRanD_join['LONG_JOIN'] = GRanD_join.LONG_GRAND\n",
    "GRanD_join.loc[GRanD_join.LAT_GRAND == -999, 'LAT_JOIN'] = GRanD_join.Latitude\n",
    "GRanD_join.loc[GRanD_join.LONG_GRAND == -999, 'LONG_JOIN'] = GRanD_join.Longitude\n",
    "\n",
    "#Drop any GRanD dams that do not have an NID\n",
    "GRanD_join = GRanD_join[GRanD_join['NID'].str.strip() != \"\"]\n",
    "\n",
    "print('Size before joining to GRanD:',NID_join.shape)\n",
    "print('Size after joining to GRanD:',GRanD_join.shape) # New database size after adding GRanD dams\n",
    "print('Number of sites in database:',GRanD_join.loc[GRanD_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a4b546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database after joining to removed dams: (90668, 111)\n"
     ]
    }
   ],
   "source": [
    "# Find matches between Removed Dams and NID by NID; outer join should keep Removed dams that weren't in NID. \n",
    "# If you do not want to use a removed dams file, skip this step.\n",
    "GRanD_join = pd.merge(GRanD_join, removed, on='NID', how='outer', suffixes = ('_join','_rem'))\n",
    "\n",
    "# Create a GRanD lat/long field that takes the lat/long value from the two joins that is not -999 (the maximum)\n",
    "GRanD_join['LAT_Rem'] = GRanD_join['DamLatitud']\n",
    "GRanD_join['LONG_Rem'] = GRanD_join['DamLongitu']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_Rem = GRanD_join.LAT_Rem.fillna(-999)\n",
    "GRanD_join.LONG_Rem = GRanD_join.LONG_Rem.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for GRanD and manually placed moved dams. This preferentially keeps their locations.\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LONG_Rem'] = -999                                                                                    \n",
    "\n",
    "GRanD_join.loc[GRanD_join.Moved == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.Moved == 1, 'LONG_Rem'] = -999\n",
    "                           \n",
    "# Create a new field for lat/long that takes the Removed dam lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join.loc[GRanD_join.LAT_Rem != -999, 'LAT_JOIN'] = GRanD_join.LAT_Rem\n",
    "GRanD_join.loc[GRanD_join.LONG_Rem != -999, 'LONG_JOIN'] = GRanD_join.LONG_Rem\n",
    "\n",
    "# Fix duplicate fields from join\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem.isnull() == True,'Batch_for_rem'] = -999 # Set null values to -999\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem != -999, 'Batch_for'] = GRanD_join.Batch_for_rem\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem == -999, 'Batch_for'] = GRanD_join.Batch_for_join\n",
    "\n",
    "print('Number of dams in database after joining to removed dams:',GRanD_join.shape) # New database size after adding removed dams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdeb3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database: (90668, 115)\n",
      "Number of locations from GeoDAR: 4282\n"
     ]
    }
   ],
   "source": [
    "# Join to GeoDAR and add GeoDAR locations where able\n",
    "NID_geoDAR = pd.merge(GRanD_join, geoDAR[['NID','lat','lon']], on='NID', how='left')\n",
    "# NID_geoDAR = pd.merge(GRanD_join, geoDAR[['NID','NewX','NewY']], on='NID', how='left')\n",
    "\n",
    "# Create a GRanD lat/long field that takes the lat/long value from the two joins that is not -999 (the maximum)\n",
    "NID_geoDAR['LAT_Geo'] = NID_geoDAR['lat']\n",
    "NID_geoDAR['LONG_Geo'] = NID_geoDAR['lon']\n",
    "# NID_geoDAR['LAT_Geo'] = NID_geoDAR['NewY_y']\n",
    "# NID_geoDAR['LONG_Geo'] = NID_geoDAR['NewX_y']\n",
    "\n",
    "# Set null values to -999\n",
    "NID_geoDAR.LAT_Geo = NID_geoDAR.LAT_Geo.fillna(-999)\n",
    "NID_geoDAR.LONG_Geo = NID_geoDAR.LONG_Geo.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for GRanD and manually placed moved dams. This preferentially keeps their locations\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LONG_Geo'] = -999                                                                                    \n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.Moved == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.Moved == 1, 'LONG_Geo'] = -999\n",
    "                                                                                    \n",
    "# Create a new field for lat/long that takes the GeoDAR lat/long if not null and takes the original lat/long if null\n",
    "NID_geoDAR.loc[NID_geoDAR.LAT_Rem != -999, 'LAT_JOIN'] = NID_geoDAR.LAT_Geo\n",
    "NID_geoDAR.loc[NID_geoDAR.LONG_Rem != -999, 'LONG_JOIN'] = NID_geoDAR.LONG_Geo\n",
    "\n",
    "print('Number of dams in database:', NID_geoDAR.shape)\n",
    "print('Number of locations from GeoDAR:', len(NID_geoDAR.loc[NID_geoDAR['LAT_Geo']!=-999]))\n",
    "\n",
    "GRanD_join = NID_geoDAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba1d76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to GDAT and get dam height, storage, and year completed data.\n",
    "\n",
    "GRanD_join = pd.merge(GRanD_join,GDAT[['NID','Year_Fin','Volume_Max','Height']],on='NID',how='left')\n",
    "GRanD_join.Year_Fin = pd.to_numeric(GRanD_join['Year_Fin'],errors='coerce') #convert string dates to integers from GDAT\n",
    "GRanD_join.Height = pd.to_numeric(GRanD_join['Height'],errors='coerce') #convert Height to integers from GDAT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69b861",
   "metadata": {},
   "source": [
    "#### Filter based on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6603f010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before storage filtering: (90668, 118)\n",
      "Size after storage filtering: (89830, 122)\n",
      "Number of sites in database: (1066, 122)\n",
      "Number of Reclamation dams in database: (257, 122)\n",
      "Number of USACE dams in database: (465, 122)\n",
      "Number of GRanD in database: (1898, 122)\n"
     ]
    }
   ],
   "source": [
    "# Assign 0 to IsGRanD non-GRanD dams\n",
    "GRanD_join.IsGRanD = GRanD_join.IsGRanD.fillna(0)\n",
    "\n",
    "# Filter based on storage\n",
    "print('Size before storage filtering:', GRanD_join.shape)\n",
    "\n",
    "# Convert fields to m3\n",
    "GRanD_join['NIDStor_m3'] = GRanD_join.NID_Storag*1233.48 #AF to m3\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM * (1e6) # Convert million cubic meters to cubic m\n",
    "GRanD_join['Volume_Max'] = GRanD_join.Volume_Max * (1e6) # Convert million cubic meters to cubic m\n",
    "\n",
    "#fill all null storage values in all fields with 0\n",
    "GRanD_join.NIDStor_m3 = GRanD_join.NIDStor_m3.fillna(0)\n",
    "GRanD_join.GRanDCapm3 = GRanD_join.GRanDCapm3.fillna(0)\n",
    "GRanD_join.CapOrig_m3 = GRanD_join.CapOrig_m3.fillna(0)\n",
    "GRanD_join.Capm3_p = GRanD_join.Capm3_p.fillna(0)\n",
    "GRanD_join.OrigCap_m3 = GRanD_join.OrigCap_m3.fillna(0)\n",
    "GRanD_join.CapNew_m3 = GRanD_join.CapNew_m3.fillna(0)\n",
    "GRanD_join.Volume_Max = GRanD_join.Volume_Max.fillna(0)\n",
    "\n",
    "# Replace NaNs with 0\n",
    "GRanD_join.IsUSBR = GRanD_join.IsUSBR.fillna(0)\n",
    "GRanD_join.IsUSACE = GRanD_join.IsUSACE.fillna(0)\n",
    "GRanD_join.IsSite = GRanD_join.IsSite.fillna(0)\n",
    "GRanD_join.IsRiverMth = GRanD_join.IsRiverMth.fillna(0)\n",
    "GRanD_join.Moved = GRanD_join.Moved.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# Create column that takes the maximum of all of the storage values for a given field. This is NID_Storag in the NID table.\n",
    "GRanD_join['MaxStor_m3'] = GRanD_join['NIDStor_m3']\n",
    "\n",
    "# Set GRanD_join storage sources to initially be NID\n",
    "# The outcome of this is that for each storage value reported, you have a source and a year that storage value represents\n",
    "GRanD_join['StorSource'] = 'NID'\n",
    "\n",
    "\n",
    "#Replace any with MaxStor == 0 with GRanD storage, dam attribute file storage, removed dams storage, then GDAT storage\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.GRanDCapm3 #GRanD\n",
    "\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.CapOrig_m3 #dam attribute file and iCold\n",
    "\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.OrigCap_m3 #Removed dams\n",
    "\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GDAT'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.Volume_Max #GDAT\n",
    "\n",
    "\n",
    "#Any dams with max-storage = 0 that is not a site or federal is removed\n",
    "GRanD_join = GRanD_join.drop(index=GRanD_join.loc[GRanD_join.MaxStor_m3 == 0].loc[GRanD_join.IsRiverMth==0].index)\n",
    "\n",
    "\n",
    "print('Size after storage filtering:',GRanD_join.shape)\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d97e93",
   "metadata": {},
   "source": [
    "#### Assign ShortIDs to any dam that doesn't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de33b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign unique ShortID to non-sites\n",
    "\n",
    "# First, sort by ascending NID\n",
    "GRanD_join = GRanD_join.sort_values('NID', ascending = True)\n",
    "\n",
    "# Starting ShortID should be the maximum of the site/GRanD ShortIDs plus 1,000 and rounded to the nearest thousandth\n",
    "startID = math.floor((GRanD_join.ShortID.max() + 1000)/1000)*1000\n",
    "ID = startID\n",
    "\n",
    "# Assign a ShortID to anything that doesn't have one yet       \n",
    "for index, row in GRanD_join.iterrows():\n",
    "    if pd.isna(row['ShortID']):  # Check if ShortID is null\n",
    "        GRanD_join.loc[index, 'ShortID'] = ID\n",
    "        ID += 1  # Increment ID for next ShortID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a074e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicate ShortIDs\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate ShortIDs. If there are duplicates, you have an error in your site input files. This is because either\n",
    "# you assigned two dams with different NIDs the same ShortID or could have a typo in the NID field.\n",
    "\n",
    "test = GRanD_join.ShortID\n",
    "\n",
    "nodup = set(test)\n",
    "\n",
    "if len(nodup) != len(test):\n",
    "    print('There are duplicate ShortIDs!')\n",
    "\n",
    "    newlist = [] # Empty list to hold unique elements from the list.\n",
    "    duplist = [] # Empty list to hold the duplicate elements from the list.\n",
    "    for i in test:\n",
    "        if i not in newlist:\n",
    "            newlist.append(i)\n",
    "        else:\n",
    "            duplist.append(i) # This method catches the first duplicate entries, and appends them to the list.\n",
    "            \n",
    "    # The next step is to print the duplicate entries, and the unique entries\n",
    "    print(\"List of duplicates\", duplist)\n",
    "else:\n",
    "    print('There are no duplicate ShortIDs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc6447",
   "metadata": {},
   "source": [
    "#### Clean up columns and export as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d63b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine columns where necessary and drop unwanted columns\n",
    "\n",
    "# Combine NID Dam height and removed dam heights into one field. Only put in removed dam where NID is null\n",
    "GRanD_join['DamH_ft'] = GRanD_join.NID_Height\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull() == True),'DamH_ft'] = GRanD_join.DAmHft #removed dam database\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.DAM_HGT_M*3.28 #GRanD\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.Height*3.28 #GDAT\n",
    "\n",
    "# Fill in Dam function from removed dam file everywhere PrimPurp is null\n",
    "# Then replace the null PrimaryPurp with Purp because some have null PrimaryPurp and non-null Purp.\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.DamFunctio\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.Purp\n",
    "\n",
    "# Year completed\n",
    "GRanD_join.loc[GRanD_join['yrc'].notna(),'yrc_source'] = GRanD_join.Batch_for #Removed dams\n",
    "GRanD_join['yrc'] = GRanD_join['yrc'].fillna(GRanD_join['Year_Compl']) # Anywhere removed dam database is null, change to NID\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'NID'\n",
    "GRanD_join.loc[(GRanD_join.Moved == 1) & (GRanD_join.Batch_for != 'iCOLD'), 'yrc'] = GRanD_join.Year_Compl_site #Anyting else still null fill from dam attributes file\n",
    "GRanD_join.loc[(GRanD_join.Moved == 1) & (GRanD_join.Batch_for != 'iCOLD'), 'yrc_source'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['YEAR'] # Anything else still null fill with GRanD\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['Year_Compl_site']# Anything else still null fill with iCOLD\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'iCOLD'\n",
    "GRanD_join.loc[GRanD_join['yrc'].isna(),'yrc'] = GRanD_join['Year_Fin']# Anything else still null fill with GDAT\n",
    "GRanD_join.loc[(GRanD_join['yrc'].notna()) & (GRanD_join['yrc_source'].isna()),'yrc_source'] = 'GDAT'\n",
    "\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull(),'yrc_source'] = np.nan #anywhere with no year completed, fill the source as NaN\n",
    "\n",
    "# Year removed\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0) #Deal with inconsistencies in datatypes in fields\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "GRanD_join['yrr'] = GRanD_join.apply(lambda row: row['YrRemoved'] if row['yrr'] == 0 else row['yrr'], axis=1) #attribute file/USACE/USBR\n",
    "\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "GRanD_join['yrr'] = GRanD_join.apply(lambda row: row['REM_YEAR'] if row['yrr'] == 0 else row['yrr'], axis=1) #GRanD\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].fillna(0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-99,0)\n",
    "GRanD_join['yrr'] = GRanD_join['yrr'].replace(-999,0)\n",
    "\n",
    "# Fill nans with 0\n",
    "GRanD_join['IsRiverMth'] = GRanD_join['IsRiverMth'].fillna(0) #River indicator\n",
    "GRanD_join['delta'] = GRanD_join['delta'].fillna(0) #delta indicator\n",
    "\n",
    "\n",
    "# Dam name\n",
    "GRanD_join.loc[GRanD_join.Dam_Name=='','Dam_Name'] = np.nan # Set values we made blank earlier back to nan\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Other_Dam # Replace missing NID names with NID other name first\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Reservoir # Then site\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.RES_NAME # Then GRanD\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.DamName # Then Removed file\n",
    "\n",
    "\n",
    "# Drop unwanted columns at this point\n",
    "GRanD_join = GRanD_join.drop(['index','Other_Dam','OtherStructureID','FederalID','Longitude','Latitude','River','Owner_Name',\n",
    "                             'Max_Storag','Normal_Sto','Lat','Long','Batch_for_join','ShortID_NID',\n",
    "                              'IsSite_NID','IsUSBR_NID','IsUSACE_NID','DAM_NAME','ALT_NAME',\n",
    "                              'ADMIN_UNIT','LONG_DD','LAT_DD','NIDnotes','ShortID_GRanD','HasNHD',\n",
    "                              'IsSite_GRanD','IsUSBR_GRanD','IsUSACE_GRanD','LAT_GRAND','LONG_GRAND',\n",
    "                              'CitationID','CitationUR','DamAccessi','DamRiverNa','DamRiver_1','DamLocatio',\n",
    "                              'DamState_P','DamLatitud','DamLongitu','DamAccurac','DamOwner','Batch_for_rem',\n",
    "                              'LAT_Rem','LONG_Rem','LAT_Geo','LONG_Geo','NID_Height','DamH_m',\n",
    "                              'DAmHft','Purp','DamFunctio','Other_Dam','REM_YEAR','RES_NAME',\n",
    "                              'DamName','DAM_HGT_M','USACE_PROJECT_ID','YEAR','CapNewAF','CapAF_p',\n",
    "                             'CAP_MCM','method','Year_Fin','Height','Volume_Max','GRanD_ID','SiteIsGRanD','OID__join',\n",
    "                             'OID__rem','lat','lon','NID_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6de26876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Year Completed fields to fill in no Year Completed data into yrc.\n",
    "\n",
    "# first, re-index\n",
    "GRanD_join = GRanD_join.reset_index(drop=True)\n",
    "\n",
    "# If yrc is outside of 1700-2023, make it 0 because is likely wrong.\n",
    "GRanD_join.loc[(GRanD_join.yrc < 1700) | (GRanD_join.yrc > 2023), 'yrc'] = 0\n",
    "\n",
    "# If yrc = 0 and Year_Modif from NID ~=0, set yrc to the minimum of Year_Modif\n",
    "for i in range(len(GRanD_join['Year_Modif'])):\n",
    "    years = GRanD_join['Year_Modif'][i]\n",
    "    \n",
    "    if isinstance(years,float):\n",
    "        if np.isnan(years):\n",
    "            years = []\n",
    "    else:\n",
    "        years = str(years)\n",
    "        years = years.split(';')\n",
    "        years = [int(re.search(r'\\d+',year).group()) for year in years]\n",
    "        \n",
    "        if len(years) > 0:\n",
    "            minyr = min(years)\n",
    "            if GRanD_join['yrc'][i] == 0:\n",
    "                GRanD_join.loc[i, 'yrc'] = minyr\n",
    "\n",
    "GRanD_join.loc[GRanD_join.yrc == -99, 'yrc'] = 0\n",
    "GRanD_join.loc[GRanD_join.yrc.isna(),'yrc'] = 0\n",
    "\n",
    "GRanD_join = GRanD_join.drop(['Year_Compl','Year_Modif','Year_Compl_site','YrRemoved'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e017d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to metric and drop the ft column\n",
    "GRanD_join['Dam_Len_m'] = GRanD_join.Dam_Length*0.3048\n",
    "GRanD_join['SA_m2'] = GRanD_join.Surface_Ar*4046.85642\n",
    "GRanD_join['DA_km2'] = GRanD_join.Drainage_A*2.58998811\n",
    "GRanD_join['MaxQ_m3s'] = GRanD_join.Max_Discha*0.028316847\n",
    "GRanD_join['DamH_m'] = GRanD_join.DamH_ft * 0.3048\n",
    "\n",
    "# Fill in nans in Moved field\n",
    "GRanD_join['Moved'] = GRanD_join['Moved'].fillna(0)\n",
    "\n",
    "# Drop columns with imperial units\n",
    "GRanD_join = GRanD_join.drop(['Dam_Length','Surface_Ar','Drainage_A','Max_Discha','CapOrigAF',\n",
    "                             'DamNameAlt','elev_ft','DamH_ft','NID_Storag'\n",
    "                              ],axis=1)\n",
    "# Rename columns\n",
    "GRanD_join.rename(columns = {'OrigCap_m3':'OCapm3_Rem','LAT_JOIN':'LAT_FINAL','LONG_JOIN':'LONG_FINAL',\n",
    "                             'DA_km':'site_DA_km'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e506bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final files as a csv\n",
    "GRanD.to_csv(os.path.join(out_folder,'GRanD.csv'))\n",
    "GRanD_join.to_csv(os.path.join(out_folder,'NID_GRanDjoin.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b01ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size moving to snapping: (89830, 45)\n",
      "Number of sites in database: (1066, 45)\n",
      "Number of Reclamation dams in database: (257, 45)\n",
      "Number of USACE dams in database: (465, 45)\n",
      "Number of GRanD in database: (1898, 45)\n"
     ]
    }
   ],
   "source": [
    "print('Size moving to snapping:',GRanD_join.shape)\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b74d07",
   "metadata": {},
   "source": [
    "# 2. Snap dams to NHDPlus Flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e590628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, November 7, 2024 8:27:21 PM\",\"NID_filtered Successfully converted:  E:/ResSed/MediumResolution_DamLinkages/Manuscript/Outputs_Nov7\\\\NID_filtered.shp\",\"Succeeded at Thursday, November 7, 2024 8:27:55 PM (Elapsed Time: 33.97 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'E:\\\\ResSed\\\\MediumResolution_DamLinkages\\\\Manuscript\\\\Outputs_Nov7'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snap dams to NHDPlus HR Flowlines: must be done with arcPy. Ensure that all layers are in the same coordinate system (here we use NAD83).\n",
    "\n",
    "# First convert the csv to a shapefile\n",
    "XFieldName = 'LONG_FINAL'\n",
    "YFieldName = 'LAT_FINAL'\n",
    "newLayerName = \"NID_filtered\" # Name of your output shapefile\n",
    "\n",
    "spatialRef = arcpy.SpatialReference(4269) # Spatial reference WKID for NAD83\n",
    "csvFilePath = os.path.join(out_folder,'NID_GRanDjoin.csv') # Your filtered dam dataset csv\n",
    "\n",
    "\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, newLayerName, spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion(newLayerName, out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c7a94-123d-41c8-8aaa-9846afb7ab18",
   "metadata": {},
   "source": [
    "*The following 2 cells will take multiple hours to run, so plan accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fd104d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total non-site initial selection: 86359\n",
      "Total snapped to flowline: 30961\n",
      "Total moving to storage >= 5e6 m$^3$ step: 1416\n",
      "Total snapped to flowline: 814\n",
      "Total moving to storage < 5e6 m$^3$ step: 53982\n",
      "Total snapped to flowline: 24930\n",
      "Total sites snapped to flowline: 1066\n",
      "Total GRanD snapped to flowline: 1791\n",
      "Total USBR snapped to flowline: 257\n",
      "Total USACE snapped to flowline: 451\n",
      "Total moved dams snapped to flowline: 2201\n",
      "Total dams that do not snap: 29787\n",
      "Total dams in DamNet: 60043\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Thursday, November 7, 2024 10:53:07 PM\",\"Succeeded at Thursday, November 7, 2024 10:53:10 PM (Elapsed Time: 2.81 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'NIDFiltered_snap'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For NHDPlus\n",
    "NIDFiltered = os.path.join(out_folder,'NID_filtered.shp') #Link to your filtered NID shapefile\n",
    "\n",
    "NIDlyr = \"NIDlyr\" #create a layer file\n",
    "NHDlyr = \"NHDlyr\"\n",
    "arcpy.management.MakeFeatureLayer(NIDFiltered,NIDlyr) #convert the feature class to a layer to work from\n",
    "arcpy.management.MakeFeatureLayer(NHDFlowline,NHDlyr)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "# Run the near tool to get the new lat/long with near FType558\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE = 55800\")\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 0\") #select all non-sites, non-Reclamation, non-USACE, and non-GRanD to snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "\n",
    "# Print counts\n",
    "print('Total non-site initial selection:',arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Near 250 m to FType558 for everything that isn't a site\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Create a new field and populate it\n",
    "arcpy.management.AddField(NIDlyr,'NrX_Final',\"DOUBLE\")\n",
    "arcpy.management.AddField(NIDlyr,'NrY_Final',\"DOUBLE\")\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline FType 558 Selection before transferring over values\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "# Near 500m to all flowlines for any that didn't snap and have MaxStor >= 4000 AF/5,0000,000 m^3\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And not GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3>=5e6\") #MaxStor >= 4000 AF\n",
    "\n",
    "# Print counts\n",
    "print('Total moving to storage >= 5e6 m$^3$ step:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"500 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline Selection before transferring over values\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# Near 250m to all flowlines for any that didn't snap and still aren't a site\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'SUBSET_SELECTION',\"DivDASqKM > 0\") #only select flowlines with > 0 sq.km.\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And any that aren't GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"Moved = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3<5e6\") #MaxStor < 4000 AF\n",
    "\n",
    "\n",
    "# Print counts\n",
    "print('Total moving to storage < 5e6 m$^3$ step:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline Selection before transferring over values\n",
    "\n",
    "# Print counts\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "######################################################################################\n",
    "# Sites to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 1\") #Select all sites\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total sites snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# GRanD to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsGRanD = 1\") #Select all GRanD\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total GRanD snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# USBR to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSBR = 1\") #Select all USBR\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total USBR snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# USACE to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSACE = 1\") #Select all USACE\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total USACE snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Moved dams to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"Moved = 1\") #Select all dams moved in the NID edit file\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "# Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"NrX_Final <> -1\")\n",
    "print('Total moved dams snapped to flowline:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# Delete any features that did not snap.\n",
    "# First select non-snaps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION')\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\")\n",
    "\n",
    "print('Total dams that do not snap:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "# Delete the selected rows:\n",
    "arcpy.management.DeleteRows(NIDlyr)\n",
    "\n",
    "print('Total dams in DamNet:', arcpy.management.GetCount(NIDlyr))\n",
    "\n",
    "\n",
    "# Display XY data using NearX and NearY to move the points onto the NHD Flowlines\n",
    "arcpy.management.MakeXYEventLayer(NIDlyr, \"NrX_Final\", \"NrY_Final\", 'NIDFiltered_snap', spatialRef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885d771",
   "metadata": {},
   "source": [
    "### Intersect with the flowline data to extract attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09de94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersect file with moved dams to the flowline file:\n",
    "arcpy.analysis.Intersect(['NIDFiltered_snap',NHDFlowline],os.path.join(out_folder,'NIDFiltered_snap.shp'),\"ALL\",None,\"INPUT\")\n",
    "\n",
    "# Export NID file\n",
    "dbf = Dbf5(os.path.join(out_folder,'NIDFiltered_snap.dbf'))\n",
    "df = dbf.to_dataframe()\n",
    "df.to_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4775fbd",
   "metadata": {},
   "source": [
    "### Remove dams snapping to duplicate flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deac67b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before duplicates removed: (76196, 189)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate flowlines. When you run intersect in ArcGIS, any intersects that happen at the join of two lines gives two\n",
    "# results in the final table. We need to delete one of these.\n",
    "\n",
    "NID = pd.read_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'), low_memory=False) # Load data.\n",
    "# Delete any that snapped to no-drainage area flowlines that aren't sites\n",
    "NID = NID.drop(index=NID.loc[NID.DivDASqKM == 0].index)\n",
    "\n",
    "NID_sort = NID.sort_values('ShortID', ascending = True) # Sort by ascending ShortID.\n",
    "\n",
    "\n",
    "print('Size before duplicates removed:', NID_sort.shape)\n",
    "\n",
    "# Convert dataframe to dictionaries (struct-like); basically has format column->value.\n",
    "NID_dict = NID_sort.to_dict(orient = 'records')\n",
    "dupl_ordered_dict = NID.to_dict(orient='records')\n",
    "\n",
    "# Initialize empty list to store indices of non-duplicates.\n",
    "dupind = []\n",
    "\n",
    "# Identify unique values and their counts.\n",
    "shortID = [item['ShortID'] for item in dupl_ordered_dict]\n",
    "uniquevals,ia = np.unique(shortID, return_inverse = True)\n",
    "\n",
    "# Count the frequency of each index in ia.\n",
    "bincounts = np.bincount(ia)\n",
    "\n",
    "# Zero out singles.\n",
    "singles = uniquevals[bincounts <= 1]\n",
    "singleidx = [i for i, val in enumerate(shortID) if val in singles]\n",
    "for idx in singleidx:\n",
    "    shortID[idx] = 0\n",
    "    \n",
    "# Overwrite repeats.\n",
    "repeats = uniquevals[bincounts > 1]\n",
    "shortID = np.array([np.where(repeats == val)[0][0] + 1 if val in repeats else val for val in shortID])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd3f9f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing duplicates: (60033, 189)\n",
      "Number of sites in database: (1066, 189)\n",
      "Number of Reclamation dams in database: (257, 189)\n",
      "Number of USACE dams in database: (451, 189)\n",
      "Number of GRanD in database: (1790, 189)\n"
     ]
    }
   ],
   "source": [
    "skip_it = 0; # Initialize a counter.\n",
    "# Pull out FCODE and Hydrosequence fields to help with decision tree for removing duplicates.\n",
    "FCODE = [item['FCODE'] for item in dupl_ordered_dict]\n",
    "Hydroseq = [item['Hydroseq'] for item in dupl_ordered_dict]\n",
    "\n",
    "\n",
    "for i in range(len(shortID)):\n",
    "    if shortID[i] == 0: # If it is not a duplicate, keep it.\n",
    "        dupind.append(i)\n",
    "    elif skip_it > 0: # Or if we already dealt with it, update the counter so it gets skipped.\n",
    "        skip_it -= 1\n",
    "        continue\n",
    "    else: # Else the value is a duplicate.\n",
    "        dup = [idx for idx, val in enumerate(shortID) if val == shortID[i]] # Gives all indices of the duplicates.\n",
    "        dup1 = dup[0]\n",
    "        dupskip = dup1 # Keep track of what the first index was because we will change this.\n",
    "        j = len(dup)\n",
    "        jskip = j # Same for the length of the duplicates.\n",
    "\n",
    "        Hydro = Hydroseq[dup1:dup[j-1]+1] # Pull out Hydrosequences as the duplicates.\n",
    "\n",
    "        kept_indices = [i for i, x in enumerate(Hydro) if x not in [Hydroseq[i] for i in dupind]] # If a dam is already snapped to that flowline, remove the flowlines from the options to choose from.\n",
    "\n",
    "        dup_test = [dup[i] for i in kept_indices]\n",
    "                \n",
    "        if len(dup_test) == 0: # All of the flowline options have already been used, in which case just keep them all. Duplicate snaps are removed later.\n",
    "            dup_test = dup\n",
    "        \n",
    "        dup = dup_test\n",
    "        \n",
    "        dup1 = dup[0]\n",
    "        j = len(dup)\n",
    "        \n",
    "        Floc = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 55800] # Pull out FCODE = 55800 for duplicates (flowlines in reservoirs).\n",
    "        coast = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 56600] # Pull out any duplicates that are on a coast flowline.\n",
    "           \n",
    "        if len(Floc) == 1: # If only one value is FType 558.\n",
    "            dupind.append(dup[Floc[0]])\n",
    "        elif len(Floc) == j: # All of the values are 558, take smallest hydroseq (most downstream).\n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[minloc])\n",
    "        elif len(Floc) == 0: # None are FType 558.         \n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "           \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast) # Remove coastal values; any dam that snaps to a coast flowline has it's dam order messed up and can route along the coast.\n",
    "                dup = np.delete(dup,coast)\n",
    "\n",
    "            minloc = np.argmin(Hydro) # Currently taking minimum of the new hydro.\n",
    "            dupind.append(dup[minloc])\n",
    "        else: # Some other number of values is FType 558; still take the most downstream.\n",
    "            Hydro = [Hydroseq[dup[index]] for index in Floc]\n",
    "    \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast)\n",
    "                dup = np.delete(dup,coast)\n",
    "                \n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[Floc[minloc]])\n",
    "            \n",
    "        if dupskip == i: # If the first index was the current index.\n",
    "            skip_it = jskip-1 # Skip the next j-1 indices.\n",
    "        else:\n",
    "            skip_it = 0\n",
    "\n",
    "dupltable = pd.DataFrame.from_dict(dupl_ordered_dict)\n",
    "noduplicates = dupltable.loc[dupind]\n",
    "\n",
    "\n",
    "\n",
    "print('Size after removing duplicates:',noduplicates.shape) # New database size after snapping to NHD flowlines\n",
    "print('Number of sites in database:', noduplicates.loc[noduplicates.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', noduplicates.loc[noduplicates.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', noduplicates.loc[noduplicates.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', noduplicates.loc[noduplicates.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0da3f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dam_Name</th>\n",
       "      <th>NID</th>\n",
       "      <th>State</th>\n",
       "      <th>OwnerTypes</th>\n",
       "      <th>PrimaryPur</th>\n",
       "      <th>PrimDamTyp</th>\n",
       "      <th>Reservoir</th>\n",
       "      <th>Year_First</th>\n",
       "      <th>Year_Last</th>\n",
       "      <th>Owner</th>\n",
       "      <th>RES_SED_No</th>\n",
       "      <th>CapOrig_m3</th>\n",
       "      <th>CapNew_m3</th>\n",
       "      <th>site_DA_km</th>\n",
       "      <th>IsRiverMth</th>\n",
       "      <th>delta</th>\n",
       "      <th>IsLock</th>\n",
       "      <th>yr_p</th>\n",
       "      <th>Capm3_p</th>\n",
       "      <th>USBRname</th>\n",
       "      <th>GRAND_ID</th>\n",
       "      <th>IsGRanD</th>\n",
       "      <th>NewX</th>\n",
       "      <th>NewY</th>\n",
       "      <th>ShortID</th>\n",
       "      <th>IsSite</th>\n",
       "      <th>IsUSBR</th>\n",
       "      <th>IsUSACE</th>\n",
       "      <th>yrc</th>\n",
       "      <th>yrr</th>\n",
       "      <th>OCapm3_Rem</th>\n",
       "      <th>Batch_for</th>\n",
       "      <th>NIDStor_m3</th>\n",
       "      <th>GRanDCapm3</th>\n",
       "      <th>MaxStor_m3</th>\n",
       "      <th>StorSource</th>\n",
       "      <th>yrc_source</th>\n",
       "      <th>Dam_Len_m</th>\n",
       "      <th>SA_m2</th>\n",
       "      <th>DA_km2</th>\n",
       "      <th>MaxQ_m3s</th>\n",
       "      <th>DamH_m</th>\n",
       "      <th>NrX_Final</th>\n",
       "      <th>NrY_Final</th>\n",
       "      <th>COMID</th>\n",
       "      <th>LENGTHKM</th>\n",
       "      <th>FCODE</th>\n",
       "      <th>Hydroseq</th>\n",
       "      <th>Pathlength</th>\n",
       "      <th>TerminalPa</th>\n",
       "      <th>DnHydroseq</th>\n",
       "      <th>DivDASqKM</th>\n",
       "      <th>SLOPE</th>\n",
       "      <th>QA_MA</th>\n",
       "      <th>VA_MA</th>\n",
       "      <th>QC_MA</th>\n",
       "      <th>VC_MA</th>\n",
       "      <th>QE_MA</th>\n",
       "      <th>VE_MA</th>\n",
       "      <th>Country_ou</th>\n",
       "      <th>WBCOMID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RioGrande</td>\n",
       "      <td>MOUTH_RGocean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RioGrande</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Rivers</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>GDAT</td>\n",
       "      <td>Rivers</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-97.146264</td>\n",
       "      <td>25.956148</td>\n",
       "      <td>626220</td>\n",
       "      <td>0.118</td>\n",
       "      <td>55800</td>\n",
       "      <td>680000035.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>680000035.0</td>\n",
       "      <td>680000002.0</td>\n",
       "      <td>449182.5912</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>18942.517</td>\n",
       "      <td>-9999.00000</td>\n",
       "      <td>17738.014</td>\n",
       "      <td>-9999.00000</td>\n",
       "      <td>11938.578</td>\n",
       "      <td>-9999.00000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>625912.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lakeway Subdivision Lake Dam</td>\n",
       "      <td>TX05842</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Private</td>\n",
       "      <td>Recreation</td>\n",
       "      <td>Earth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>363053.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150484.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150484.56</td>\n",
       "      <td>NID</td>\n",
       "      <td>NID</td>\n",
       "      <td>27.432</td>\n",
       "      <td>72843.41556</td>\n",
       "      <td>0.802896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4384</td>\n",
       "      <td>-97.531512</td>\n",
       "      <td>25.978205</td>\n",
       "      <td>207851</td>\n",
       "      <td>26.132</td>\n",
       "      <td>46006</td>\n",
       "      <td>630010377.0</td>\n",
       "      <td>27.862</td>\n",
       "      <td>630007866.0</td>\n",
       "      <td>630010198.0</td>\n",
       "      <td>255.8484</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>9.802</td>\n",
       "      <td>0.82906</td>\n",
       "      <td>20.606</td>\n",
       "      <td>0.86136</td>\n",
       "      <td>20.606</td>\n",
       "      <td>0.86136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Viejo Dam D</td>\n",
       "      <td>TX06232</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Private</td>\n",
       "      <td>Irrigation</td>\n",
       "      <td>Earth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>363421.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>314537.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>314537.40</td>\n",
       "      <td>NID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.960</td>\n",
       "      <td>263045.66730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4384</td>\n",
       "      <td>-97.553501</td>\n",
       "      <td>25.997315</td>\n",
       "      <td>207873</td>\n",
       "      <td>1.571</td>\n",
       "      <td>55800</td>\n",
       "      <td>630010791.0</td>\n",
       "      <td>57.627</td>\n",
       "      <td>630007866.0</td>\n",
       "      <td>630010579.0</td>\n",
       "      <td>142.4178</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>5.330</td>\n",
       "      <td>-9998.00000</td>\n",
       "      <td>12.530</td>\n",
       "      <td>-9998.00000</td>\n",
       "      <td>12.530</td>\n",
       "      <td>-9998.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>207583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rancho Viejo Dam C</td>\n",
       "      <td>TX06231</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Private</td>\n",
       "      <td>Irrigation</td>\n",
       "      <td>Earth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>363420.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>370044.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>370044.00</td>\n",
       "      <td>NID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.960</td>\n",
       "      <td>263045.66730</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1336</td>\n",
       "      <td>-97.540140</td>\n",
       "      <td>26.002529</td>\n",
       "      <td>206919</td>\n",
       "      <td>4.629</td>\n",
       "      <td>46006</td>\n",
       "      <td>630011557.0</td>\n",
       "      <td>61.425</td>\n",
       "      <td>630007866.0</td>\n",
       "      <td>630011279.0</td>\n",
       "      <td>117.1035</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>4.376</td>\n",
       "      <td>0.59582</td>\n",
       "      <td>10.667</td>\n",
       "      <td>0.61733</td>\n",
       "      <td>10.667</td>\n",
       "      <td>0.61733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rancho Viejo Dam B</td>\n",
       "      <td>TX06230</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Private</td>\n",
       "      <td>Irrigation</td>\n",
       "      <td>Earth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>402030.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>185022.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>185022.00</td>\n",
       "      <td>NID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.960</td>\n",
       "      <td>303514.23150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1336</td>\n",
       "      <td>-97.538459</td>\n",
       "      <td>26.015948</td>\n",
       "      <td>206919</td>\n",
       "      <td>4.629</td>\n",
       "      <td>46006</td>\n",
       "      <td>630011557.0</td>\n",
       "      <td>61.425</td>\n",
       "      <td>630007866.0</td>\n",
       "      <td>630011279.0</td>\n",
       "      <td>117.1035</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>4.376</td>\n",
       "      <td>0.59582</td>\n",
       "      <td>10.667</td>\n",
       "      <td>0.61733</td>\n",
       "      <td>10.667</td>\n",
       "      <td>0.61733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Dam_Name            NID  State OwnerTypes  PrimaryPur  \\\n",
       "0                     RioGrande  MOUTH_RGocean    NaN        NaN         NaN   \n",
       "1  Lakeway Subdivision Lake Dam        TX05842  Texas    Private  Recreation   \n",
       "3            Rancho Viejo Dam D        TX06232  Texas    Private  Irrigation   \n",
       "4            Rancho Viejo Dam C        TX06231  Texas    Private  Irrigation   \n",
       "5            Rancho Viejo Dam B        TX06230  Texas    Private  Irrigation   \n",
       "\n",
       "  PrimDamTyp  Reservoir  Year_First  Year_Last Owner RES_SED_No  CapOrig_m3  \\\n",
       "0        NaN  RioGrande         0.0        0.0   NaN        NaN         0.0   \n",
       "1      Earth        NaN         NaN        NaN   NaN        NaN         0.0   \n",
       "3      Earth        NaN         NaN        NaN   NaN        NaN         0.0   \n",
       "4      Earth        NaN         NaN        NaN   NaN        NaN         0.0   \n",
       "5      Earth        NaN         NaN        NaN   NaN        NaN         0.0   \n",
       "\n",
       "   CapNew_m3  site_DA_km  IsRiverMth  delta  IsLock  yr_p  Capm3_p USBRname  \\\n",
       "0        0.0         0.0         1.0   22.0     0.0   NaN      0.0      NaN   \n",
       "1        0.0         NaN         0.0    0.0     0.0   NaN      0.0      NaN   \n",
       "3        0.0         NaN         0.0    0.0     0.0   NaN      0.0      NaN   \n",
       "4        0.0         NaN         0.0    0.0     0.0   NaN      0.0      NaN   \n",
       "5        0.0         NaN         0.0    0.0     0.0   NaN      0.0      NaN   \n",
       "\n",
       "   GRAND_ID  IsGRanD  NewX  NewY   ShortID  IsSite  IsUSBR  IsUSACE     yrc  \\\n",
       "0       NaN      0.0   NaN   NaN      -9.0     1.0     0.0      0.0  1700.0   \n",
       "1       NaN      0.0   NaN   NaN  363053.0     0.0     0.0      0.0  1981.0   \n",
       "3       NaN      0.0   NaN   NaN  363421.0     0.0     0.0      0.0     0.0   \n",
       "4       NaN      0.0   NaN   NaN  363420.0     0.0     0.0      0.0     0.0   \n",
       "5       NaN      0.0   NaN   NaN  402030.0     0.0     0.0      0.0     0.0   \n",
       "\n",
       "   yrr  OCapm3_Rem Batch_for  NIDStor_m3  GRanDCapm3  MaxStor_m3 StorSource  \\\n",
       "0  0.0         0.0    Rivers        0.00         0.0        0.00       GDAT   \n",
       "1  0.0         0.0       NaN   150484.56         0.0   150484.56        NID   \n",
       "3  0.0         0.0       NaN   314537.40         0.0   314537.40        NID   \n",
       "4  0.0         0.0       NaN   370044.00         0.0   370044.00        NID   \n",
       "5  0.0         0.0       NaN   185022.00         0.0   185022.00        NID   \n",
       "\n",
       "  yrc_source  Dam_Len_m         SA_m2    DA_km2  MaxQ_m3s  DamH_m  NrX_Final  \\\n",
       "0     Rivers      0.000       0.00000  0.000000       0.0  0.0000 -97.146264   \n",
       "1        NID     27.432   72843.41556  0.802896       0.0  2.4384 -97.531512   \n",
       "3        NaN     60.960  263045.66730  0.000000       0.0  2.4384 -97.553501   \n",
       "4        NaN     60.960  263045.66730  0.000000       0.0  2.1336 -97.540140   \n",
       "5        NaN     60.960  303514.23150  0.000000       0.0  2.1336 -97.538459   \n",
       "\n",
       "   NrY_Final   COMID  LENGTHKM  FCODE     Hydroseq  Pathlength   TerminalPa  \\\n",
       "0  25.956148  626220     0.118  55800  680000035.0       0.000  680000035.0   \n",
       "1  25.978205  207851    26.132  46006  630010377.0      27.862  630007866.0   \n",
       "3  25.997315  207873     1.571  55800  630010791.0      57.627  630007866.0   \n",
       "4  26.002529  206919     4.629  46006  630011557.0      61.425  630007866.0   \n",
       "5  26.015948  206919     4.629  46006  630011557.0      61.425  630007866.0   \n",
       "\n",
       "    DnHydroseq    DivDASqKM     SLOPE      QA_MA       VA_MA      QC_MA  \\\n",
       "0  680000002.0  449182.5912  0.000010  18942.517 -9999.00000  17738.014   \n",
       "1  630010198.0     255.8484  0.000134      9.802     0.82906     20.606   \n",
       "3  630010579.0     142.4178  0.000751      5.330 -9998.00000     12.530   \n",
       "4  630011279.0     117.1035  0.000010      4.376     0.59582     10.667   \n",
       "5  630011279.0     117.1035  0.000010      4.376     0.59582     10.667   \n",
       "\n",
       "        VC_MA      QE_MA       VE_MA  Country_ou   WBCOMID  \n",
       "0 -9999.00000  11938.578 -9999.00000         5.0  625912.0  \n",
       "1     0.86136     20.606     0.86136         0.0       0.0  \n",
       "3 -9998.00000     12.530 -9998.00000         0.0  207583.0  \n",
       "4     0.61733     10.667     0.61733         0.0       0.0  \n",
       "5     0.61733     10.667     0.61733         0.0       0.0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noduplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b6076979",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.merge(noduplicates,sites[['NID','Moved','elev_ft']],on='NID',how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddf91e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "noduplicates = test.copy()\n",
    "noduplicates['Moved'] = noduplicates.Moved.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5d7ec975",
   "metadata": {},
   "outputs": [],
   "source": [
    "noduplicates['elev_m'] = noduplicates['elev_ft']* 0.3048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e1d37f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['FID_NID_fi', 'Field1', 'FID_NHDFlo', 'FDATE', 'RESOLUTION', 'GNIS_ID', 'GNIS_NAME', 'REACHCODE', 'FLOWDIR', 'FTYPE', 'StreamLeve', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode', 'LevelPathI', 'StartFlag', 'TerminalFl', 'DnLevel', 'UpLevelPat', 'UpHydroseq', 'DnLevelPat', 'DnMinorHyd', 'DnDrainCou', 'FromMeas', 'ToMeas', 'RtnDiv', 'VPUIn', 'VPUOut', 'Tidal', 'TOTMA', 'WBAreaType', 'PathTimeMA', 'HWNodeSqKM', 'MAXELEVRAW', 'MINELEVRAW', 'MAXELEVSMO', 'MINELEVSMO', 'HWTYPE', 'SLOPELENKM', 'QA_01', 'VA_01', 'QC_01', 'VC_01', 'QE_01', 'VE_01', 'QA_02', 'VA_02', 'QC_02', 'VC_02', 'QE_02', 'VE_02', 'QA_03', 'VA_03', 'QC_03', 'VC_03', 'QE_03', 'VE_03', 'QA_04', 'VA_04', 'QC_04', 'VC_04', 'QE_04', 'VE_04', 'QA_05', 'VA_05', 'QC_05', 'VC_05', 'QE_05', 'VE_05', 'QA_06', 'VA_06', 'QC_06', 'VC_06', 'QE_06', 'VE_06', 'QA_07', 'VA_07', 'QC_07', 'VC_07', 'QE_07', 'VE_07', 'QA_08', 'VA_08', 'QC_08', 'VC_08', 'QE_08', 'VE_08', 'QA_09', 'VA_09', 'QC_09', 'VC_09', 'QE_09', 'VE_09', 'QA_10', 'VA_10', 'QC_10', 'VC_10', 'QE_10', 'VE_10', 'QA_11', 'VA_11', 'QC_11', 'VC_11', 'QE_11', 'VE_11', 'QA_12', 'VA_12', 'QC_12', 'VC_12', 'QE_12', 'VE_12', 'LakeFract', 'SurfArea', 'RAreaHLoad', 'RPUID', 'VPUID', 'Enabled', 'Shape_Leng', 'WBAREACOMI', 'Divergence', 'ArbolateSu', 'ELEVFIXED', 'TotDASqKM', 'AreaSqKM', 'LAT_FINAL', 'LONG_FINAL'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3538197566.py\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Re-order fields and drop new additions we do not want\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Re-order fields and drop new additions we do not want\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m noduplicates = noduplicates.drop(['FID_NID_fi','Field1','FID_NHDFlo','FDATE','RESOLUTION','GNIS_ID','GNIS_NAME','REACHCODE',\n\u001b[0m\u001b[0;32m      4\u001b[0m                                  \u001b[1;34m'FLOWDIR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FTYPE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'StreamLeve'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'StreamOrde'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'StreamCalc'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'FromNode'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ToNode'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'LevelPathI'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                  \u001b[1;34m'StartFlag'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'TerminalFl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DnLevel'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'UpLevelPat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'UpHydroseq'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DnLevelPat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DnMinorHyd'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DnDrainCou'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5566\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[1;36m1.0\u001b[0m     \u001b[1;36m0.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m         \"\"\"\n\u001b[1;32m-> 5568\u001b[1;33m         return super().drop(\n\u001b[0m\u001b[0;32m   5569\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5570\u001b[0m             \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4783\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4784\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4785\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4825\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4826\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4827\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4828\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7069\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7070\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7072\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['FID_NID_fi', 'Field1', 'FID_NHDFlo', 'FDATE', 'RESOLUTION', 'GNIS_ID', 'GNIS_NAME', 'REACHCODE', 'FLOWDIR', 'FTYPE', 'StreamLeve', 'StreamOrde', 'StreamCalc', 'FromNode', 'ToNode', 'LevelPathI', 'StartFlag', 'TerminalFl', 'DnLevel', 'UpLevelPat', 'UpHydroseq', 'DnLevelPat', 'DnMinorHyd', 'DnDrainCou', 'FromMeas', 'ToMeas', 'RtnDiv', 'VPUIn', 'VPUOut', 'Tidal', 'TOTMA', 'WBAreaType', 'PathTimeMA', 'HWNodeSqKM', 'MAXELEVRAW', 'MINELEVRAW', 'MAXELEVSMO', 'MINELEVSMO', 'HWTYPE', 'SLOPELENKM', 'QA_01', 'VA_01', 'QC_01', 'VC_01', 'QE_01', 'VE_01', 'QA_02', 'VA_02', 'QC_02', 'VC_02', 'QE_02', 'VE_02', 'QA_03', 'VA_03', 'QC_03', 'VC_03', 'QE_03', 'VE_03', 'QA_04', 'VA_04', 'QC_04', 'VC_04', 'QE_04', 'VE_04', 'QA_05', 'VA_05', 'QC_05', 'VC_05', 'QE_05', 'VE_05', 'QA_06', 'VA_06', 'QC_06', 'VC_06', 'QE_06', 'VE_06', 'QA_07', 'VA_07', 'QC_07', 'VC_07', 'QE_07', 'VE_07', 'QA_08', 'VA_08', 'QC_08', 'VC_08', 'QE_08', 'VE_08', 'QA_09', 'VA_09', 'QC_09', 'VC_09', 'QE_09', 'VE_09', 'QA_10', 'VA_10', 'QC_10', 'VC_10', 'QE_10', 'VE_10', 'QA_11', 'VA_11', 'QC_11', 'VC_11', 'QE_11', 'VE_11', 'QA_12', 'VA_12', 'QC_12', 'VC_12', 'QE_12', 'VE_12', 'LakeFract', 'SurfArea', 'RAreaHLoad', 'RPUID', 'VPUID', 'Enabled', 'Shape_Leng', 'WBAREACOMI', 'Divergence', 'ArbolateSu', 'ELEVFIXED', 'TotDASqKM', 'AreaSqKM', 'LAT_FINAL', 'LONG_FINAL'] not found in axis\""
     ]
    }
   ],
   "source": [
    "#Re-order fields and drop new additions we do not want\n",
    "#Re-order fields and drop new additions we do not want\n",
    "noduplicates = noduplicates.drop(['FID_NID_fi','Field1','FID_NHDFlo','FDATE','RESOLUTION','GNIS_ID','GNIS_NAME','REACHCODE',\n",
    "                                 'FLOWDIR','FTYPE','StreamLeve','StreamOrde','StreamCalc','FromNode','ToNode','LevelPathI',\n",
    "                                 'StartFlag','TerminalFl','DnLevel','UpLevelPat','UpHydroseq','DnLevelPat','DnMinorHyd','DnDrainCou',\n",
    "                                 'FromMeas','ToMeas','RtnDiv','VPUIn','VPUOut','Tidal','TOTMA','WBAreaType','PathTimeMA',\n",
    "                                 'HWNodeSqKM','MAXELEVRAW','MINELEVRAW','MAXELEVSMO','MINELEVSMO','HWTYPE','SLOPELENKM',\n",
    "                                 'QA_01','VA_01','QC_01','VC_01','QE_01','VE_01','QA_02','VA_02','QC_02','VC_02','QE_02','VE_02',\n",
    "                                 'QA_03','VA_03','QC_03','VC_03','QE_03','VE_03','QA_04','VA_04','QC_04','VC_04','QE_04','VE_04',\n",
    "                                 'QA_05','VA_05','QC_05','VC_05','QE_05','VE_05','QA_06','VA_06','QC_06','VC_06','QE_06','VE_06',\n",
    "                                 'QA_07','VA_07','QC_07','VC_07','QE_07','VE_07','QA_08','VA_08','QC_08','VC_08','QE_08','VE_08',\n",
    "                                 'QA_09','VA_09','QC_09','VC_09','QE_09','VE_09','QA_10','VA_10','QC_10','VC_10','QE_10','VE_10',\n",
    "                                 'QA_11','VA_11','QC_11','VC_11','QE_11','VE_11','QA_12','VA_12','QC_12','VC_12','QE_12','VE_12',\n",
    "                                 'LakeFract','SurfArea','RAreaHLoad','RPUID','VPUID','Enabled','Shape_Leng','WBAREACOMI',\n",
    "                                 'Divergence','ArbolateSu','ELEVFIXED','TotDASqKM','AreaSqKM','LAT_FINAL',\n",
    "                                 'LONG_FINAL'],axis=1)\n",
    "\n",
    "noduplicates = noduplicates[['Dam_Name','ShortID', 'NID', 'GRAND_ID', 'IsSite', 'IsUSBR', 'IsUSACE', 'IsGRanD', 'State', 'OwnerTypes', \n",
    "        'PrimaryPur', 'PrimDamTyp', 'Reservoir', 'Year_First', 'Year_Last', 'Owner', 'RES_SED_No', 'CapOrig_m3', \n",
    "        'CapNew_m3', 'site_DA_km', 'IsRiverMth', 'delta', 'IsLock', 'yr_p','Capm3_p', 'USBRname', 'yrc', 'yrr', 'yrc_source',\n",
    "        'OCapm3_Rem', 'Batch_for', 'NIDStor_m3', 'GRanDCapm3', 'MaxStor_m3', 'StorSource', 'Dam_Len_m', \n",
    "        'SA_m2', 'DA_km2', 'MaxQ_m3s', 'DamH_m', 'elev_m', 'NrX_Final', 'NrY_Final', 'COMID', 'LENGTHKM', 'FCODE', 'Hydroseq', \n",
    "        'Pathlength', 'DnHydroseq', 'DivDASqKM', 'SLOPE', 'QA_MA', 'VA_MA', 'QC_MA', 'VC_MA', 'QE_MA', 'VE_MA', 'Country_ou', \n",
    "        'WBCOMID','TerminalPa','Moved']]\n",
    "\n",
    "#convert cfs to cms and ft/s to m/s\n",
    "noduplicates['QA_MA'] = noduplicates['QA_MA'] * 0.0283168\n",
    "noduplicates['QC_MA'] = noduplicates['QC_MA'] * 0.0283168\n",
    "noduplicates['QE_MA'] = noduplicates['QE_MA'] * 0.0283168\n",
    "noduplicates['VA_MA'] = noduplicates['VA_MA'] * 0.3048\n",
    "noduplicates['VC_MA'] = noduplicates['VC_MA'] * 0.3048\n",
    "noduplicates['VE_MA'] = noduplicates['VE_MA'] * 0.3048\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eefb71e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['QA_MA'] = noduplicates['QA_MA'] * 0.0283168\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['QC_MA'] = noduplicates['QC_MA'] * 0.0283168\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['QE_MA'] = noduplicates['QE_MA'] * 0.0283168\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['VA_MA'] = noduplicates['VA_MA'] * 0.3048\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['VC_MA'] = noduplicates['VC_MA'] * 0.3048\n",
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_28488\\3973642187.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  noduplicates['VE_MA'] = noduplicates['VE_MA'] * 0.3048\n"
     ]
    }
   ],
   "source": [
    "noduplicates = noduplicates[['Dam_Name','ShortID', 'NID', 'GRAND_ID', 'IsSite', 'IsUSBR', 'IsUSACE', 'IsGRanD', 'State', 'OwnerTypes', \n",
    "        'PrimaryPur', 'PrimDamTyp', 'Reservoir', 'Year_First', 'Year_Last', 'Owner', 'RES_SED_No', 'CapOrig_m3', \n",
    "        'CapNew_m3', 'site_DA_km', 'IsRiverMth', 'delta', 'IsLock', 'yr_p','Capm3_p', 'USBRname', 'yrc', 'yrr', 'yrc_source',\n",
    "        'OCapm3_Rem', 'Batch_for', 'NIDStor_m3', 'GRanDCapm3', 'MaxStor_m3', 'StorSource', 'Dam_Len_m', \n",
    "        'SA_m2', 'DA_km2', 'MaxQ_m3s', 'DamH_m', 'elev_m', 'NrX_Final', 'NrY_Final', 'COMID', 'LENGTHKM', 'FCODE', 'Hydroseq', \n",
    "        'Pathlength', 'DnHydroseq', 'DivDASqKM', 'SLOPE', 'QA_MA', 'VA_MA', 'QC_MA', 'VC_MA', 'QE_MA', 'VE_MA', 'Country_ou', \n",
    "        'WBCOMID','TerminalPa','Moved']]\n",
    "\n",
    "#convert cfs to cms and ft/s to m/s\n",
    "noduplicates['QA_MA'] = noduplicates['QA_MA'] * 0.0283168\n",
    "noduplicates['QC_MA'] = noduplicates['QC_MA'] * 0.0283168\n",
    "noduplicates['QE_MA'] = noduplicates['QE_MA'] * 0.0283168\n",
    "noduplicates['VA_MA'] = noduplicates['VA_MA'] * 0.3048\n",
    "noduplicates['VC_MA'] = noduplicates['VC_MA'] * 0.3048\n",
    "noduplicates['VE_MA'] = noduplicates['VE_MA'] * 0.3048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e2b552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after joining D50: (60033, 62)\n"
     ]
    }
   ],
   "source": [
    "#D50 Data\n",
    "D50 = pd.read_csv(r\"E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\FinalInputFiles\\NHDPlus_MediumResolution_D50.csv\",header=0)\n",
    "\n",
    "NID_D50 = pd.merge(noduplicates, D50, on='COMID', how='left')\n",
    "NID_D50 = NID_D50.drop(['OID_','StreamOrde','TotDASqKM'],axis=1)\n",
    "\n",
    "print('Size after joining D50:',NID_D50.shape)\n",
    "\n",
    "NID_D50.to_csv(os.path.join(out_folder,'NID_filtered_snapped_nodupl_D50.csv'),index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# damnet = pd.read_csv(r\"E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\Outputs_Nov\\NIDsMappedInBasins_MedRes_110424.csv\")\n",
    "# sites = pd.read_csv(r\"E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\FinalInputFiles\\sites.csv\")\n",
    "# test = pd.merge(damnet,sites[['ShortID','elev_ft','Year_First','Year_Last','RES_SED_No','Owner','CapOrig_m3','CapNew_m3','DA_km','yr_p','Capm3_p','USBRname']],how='left',on='ShortID')\n",
    "\n",
    "# test = test.rename(columns={'DA_km':'site_DA_km'})\n",
    "# test['elev_m'] = test['elev_ft']*.3048\n",
    "# test = test.drop(['elev_ft'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locks = pd.read_csv(r\"T:\\Jobs\\DO\\_NonFeature\\WaterSMART\\2020-PRG-ReservoirSedimentationEquations\\DATA\\GIS\\LockDamsStorage_export102924.csv\")\n",
    "\n",
    "# test = pd.merge(test,locks[['NID','PermStorag']],how='left',on='NID')\n",
    "\n",
    "# test.to_csv(r'E:\\ResSed\\MediumResolution_DamLinkages\\Manuscript\\Outputs_Nov\\NIDsMappedInBasins_MedRes_110424_locks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4377166f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
