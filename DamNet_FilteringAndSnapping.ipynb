{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10745787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written 8/11/2022 #####(UPDATE DATE?) to automate the filtering process and assign dam order to all dams in the nation.\n",
    "# The starting file for this script is a csv file of the NID database dams for the entire nation \n",
    "# downloaded from https://nid.sec.usace.army.mil/#/downloads. You should be able to run this for only a subset\n",
    "# of the dams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce31f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AE COMMENTS (ALL COMMENTS TO DEAL WITH MARKED WITH 5 HASHTAGES):\n",
    "##### could use a description of the programs and version numbers used to run this code\n",
    "##### for example, people need to have an arcgispro installation to use arcpy package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efda67",
   "metadata": {},
   "source": [
    "# 1. Import databases and filter them. Combine databases as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876b39f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from simpledbf import Dbf5\n",
    "import os\n",
    "\n",
    "# Import arcpy packages\n",
    "import archook\n",
    "arcgis_python_path = r'C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3' #The path on your computer where your ArcGIS Python installation is located\n",
    "\n",
    "# Add the Python environment to the path\n",
    "archook.arcpy = arcgis_python_path\n",
    "\n",
    "# Locate arcgis and access arcpy\n",
    "archook.get_arcpy(pro=True) # pro=True argument may not be needed depending on archook version. If so, use:\n",
    "# archook.get_arcpy() \n",
    "import arcpy\n",
    "\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227bffc-8318-4308-bf93-bd9d14cd4472",
   "metadata": {},
   "source": [
    "*You may need to clone your arcgis environment and run from this activated environment for the line <archook.get_arcpy(pro=True)> to run. Otherwise you may receive an ImportError. More info on archook found here: https://pypi.org/project/archook-dbc/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae555cdc",
   "metadata": {},
   "source": [
    "#### Load data here. You will need to replace all of the input files with the locations of your copies of the file. The output_folder variable is the folder where all of the output files will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6357611",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data. See the files included with the report for formatting.\n",
    "\n",
    "# Load NID data\n",
    "NIDs = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID2024.csv', header=1, low_memory=False) \n",
    "\n",
    "# Load removed dams file, if using\n",
    "removed = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/RemovedDams.csv')\n",
    "\n",
    "# Load spatial edits file if manually have changed any locations or deleted dams from NID\n",
    "editNID = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID_spatialEdits.csv') #cross reference with deleting S001 etc\n",
    "\n",
    "# Load GeoDAR data to use as location where no NID location #####(ADD GEODAR VERSION)\n",
    "geoDAR = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GeoDAR_crossref.csv')\n",
    "\n",
    "# Load Federal dam and site combined data for if you have sites with additional data that you want included. These dams\n",
    "# have been manually placed on flowlines and have accurate locations.\n",
    "sites = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/sites.csv') \n",
    "\n",
    "# Load NHD Plus Medium Resolution flowline shapefile #####(ADD NHD VERSION)\n",
    "NHDFlowline = 'D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NHDFlowline_Network_NHDPlus_Countries.gdb/NHDFlowline_Network_NHDPlus_Countries'  # Path for NHD flowline shapefile\n",
    "\n",
    "# Load GRanD data. We are using v1.3 with some modifications to locations that places GRanD on NHD Flowlines.\n",
    "GRanD = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GRanD_dams_v1_3.csv')\n",
    "\n",
    "# Assign output folder directory where you want to save the output files.\n",
    "out_folder = 'D:/ResSed/MediumResolution_DamLinkages/Manuscript/Outputs' # Write full path to this folder for arcgis outputs to be saved properly\n",
    "\n",
    "# Print original numbers of dams in various databases for tracking:\n",
    "print('Original number of NID in database:',NIDs.shape)\n",
    "print('Number of USBR in database:',sites.loc[sites.IsUSBR==1].shape)\n",
    "print('Number of USACE in database:',sites.loc[sites.IsUSACE==1].shape)\n",
    "print('Number of sites in database:',sites.loc[sites.IsSite==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many dams have lat/long as 0\n",
    "noLoc = NIDs[(NIDs.Latitude == 0) | (NIDs.Longitude == 0)]\n",
    "\n",
    "print('Number of raw NIDs with no locations:', noLoc.shape)\n",
    "#noLoc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16571b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NID data to only keep the columns that we need\n",
    "# FOR 2024 NID FILE: Column headers may change between versions of NID downloads. Check this if re-running with a new NID download\n",
    "\n",
    "# Rename columns\n",
    "new_cols = ['Dam_Name','Other_Dam','Former_Name','NID','OtherStructureID','FederalID','Owner_Name','OwnerTypes','PrimaryOwnerType','NumStruct','AssStruct','Designer','NonFedDam','PrimaryPurp', 'Purp','SourceAgency','StateorFedID','Latitude','Longitude','State','County','City','DisttoCity','River','CongressDist','AmInd','SecLoc','StateReg','Juris','Agency','StatePerm','StateInsp','StateEnforce','FedReg','FedOwner','FedFunding','FedDesign','FedConst','FedReg','FedInsp','FedOps','FedOther','SecAg','NRCS','PrimDamType','DamTypes','CoreTypes','Foundation','Dam_Height','HydraulicHeight','StructHeight','NID_Height','NIDHeightCat','Dam_Length','Volume','Year_Compl','YearCompCat','Year_Modif','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','SpillwayType','SpillWidth','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','OutletGate','DataUpdated','LastInspection','InspectionFreq','HazardClass','CondAss','CondAssDate','OpStat','OpStatDate','EAPPrep','EAPRev','InundationMap','URL']\n",
    "NIDs.columns = new_cols\n",
    "\n",
    "\n",
    "# Filter and rename variables\n",
    "NIDs = NIDs[['Dam_Name','Other_Dam','NID','OtherStructureID','FederalID','Longitude','Latitude','State','River','Owner_Name','Year_Compl','Year_Modif','NID_Height','Dam_Length','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','PrimaryPurp','Purp','PrimDamType','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb5854",
   "metadata": {},
   "source": [
    "#### Move and delete NID dams using the spatial edits file if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26833d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use spatial edit file to change lat/long of moved dams and remove deleted dams. If you do not wish to modify original NID, skip this.\n",
    "delete = editNID.loc[editNID.Deleted == 1]\n",
    "move = editNID.loc[editNID.Moved == 1]\n",
    "\n",
    "NIDs = NIDs[~NIDs['NID'].isin(delete['NID_ID'])] # Drop deleted NIDs\n",
    "\n",
    "NIDs = pd.merge(NIDs, move, left_on = 'NID', right_on = 'NID_ID', how = 'left') # Add moved NIDs to NID dataframe\n",
    "\n",
    "# Change the Latitude and Longitude fields of the NIDs you wish to move\n",
    "NIDs.loc[NIDs.Moved == 1,'Latitude'] = NIDs.lat \n",
    "NIDs.loc[NIDs.Moved == 1, 'Longitude'] = NIDs.long \n",
    "\n",
    "print('Length to delete:', delete.shape)\n",
    "print('NID database size:',NIDs.shape) # New database size after incorporating spatial edits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3b4b49",
   "metadata": {},
   "source": [
    "#### Join in the site file that has sites with additional data if using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81756bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the site data to the NID database\n",
    "NID_join = pd.merge(NIDs, sites, on='NID', how='outer')\n",
    "\n",
    "# Replace Storage NaNs with 0\n",
    "NID_join.NID_Storag = NID_join.NID_Storag.fillna(0)\n",
    "NID_join.Max_Storag = NID_join.Max_Storag.fillna(0)\n",
    "NID_join.Normal_Sto = NID_join.Normal_Sto.fillna(0)\n",
    "NID_join.IsUSBR = NID_join.IsUSBR.fillna(0)\n",
    "NID_join.IsUSACE = NID_join.IsUSACE.fillna(0)\n",
    "NID_join.IsSite = NID_join.IsSite.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed08c91a",
   "metadata": {},
   "source": [
    "#### Remove duplicate NIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27969ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate NIDs. Keep dam with largest reported storage data.\n",
    "NIDs_join = NID_join.drop(index=NID_join.loc[NID_join.OtherStructureID.notnull()].index)\n",
    "\n",
    "# A couple don't have Other Structure ID but are duplicates. Filter those by storage. Should only be 2\n",
    "# Sort the data by descending max storage\n",
    "NID_join = NID_join.sort_values('NID_Storag', ascending = False)\n",
    "\n",
    "# Remove duplicate NIDs, keeping the first value aka the biggest capacity\n",
    "bool_series = NID_join['NID'].duplicated()\n",
    "NID_join = NID_join[~bool_series]\n",
    "\n",
    "# Print checks\n",
    "print('Size after removing duplicates and joining to site file:',NID_join.shape) # New database size after removing duplicate NIDs\n",
    "print('Number of sites in database:',NID_join.loc[NID_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891c7e2",
   "metadata": {},
   "source": [
    "#### Update latitude and longitude based on known lat/long from the site file and populate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18552994",
   "metadata": {},
   "outputs": [],
   "source": [
    "NID_join = NID_join.reset_index()\n",
    "\n",
    "# Set lat and long for sites to be the site lat/long\n",
    "NID_join.loc[NID_join.IsSite == 1,'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsSite == 1, 'Longitude'] = NID_join.Long\n",
    "\n",
    "# Set lat and long for Reclamation dams\n",
    "NID_join.loc[NID_join.IsUSBR == 1, 'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsUSBR == 1, 'Longitude'] = NID_join.Long\n",
    "\n",
    "# Set lat and long for Army corps dams\n",
    "NID_join.loc[NID_join.IsUSACE == 1, 'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsUSACE == 1, 'Longitude'] = NID_join.Long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991791f-db68-414d-935d-c80df0d41334",
   "metadata": {},
   "source": [
    "#### Locate and populate lock dams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4f551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate IsLock column\n",
    "# Replace null values with 0\n",
    "# Assign 1 to any NID field with lock information\n",
    "NID_join.NumLocks = NID_join.NumLocks.fillna(0)\n",
    "NID_join.LengthLocks = NID_join.LengthLocks.fillna(0)\n",
    "NID_join.LockWidth = NID_join.LockWidth.fillna(0)\n",
    "NID_join.LengthSecondLock = NID_join.LengthSecondLock.fillna(0)\n",
    "NID_join.SecondLockWidth = NID_join.SecondLockWidth.fillna(0)\n",
    "NID_join.loc[(NID_join.NumLocks>0) & (NID_join.NumLocks<10),'IsLock'] = 1 \n",
    "NID_join.loc[(NID_join.LengthLocks>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LockWidth>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LengthSecondLock>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.SecondLockWidth>0),'IsLock'] = 1\n",
    "\n",
    "# Search by name containing word 'Lock'\n",
    "NID_join['Dam_Name'] = NID_join['Dam_Name'].fillna('') # First fill NA names with empty strings\n",
    "NID_join.loc[NID_join['Dam_Name'].str.contains('Lock '), 'IsLock'] = 1 \n",
    "##### DO WE NEED TO SEARCH FOR LOWERCASE 'lock'? If so:\n",
    "##### NID_join.loc[NID_join['Dam_Name'].str.contains('lock '), 'IsLock'] = 1 \n",
    "\n",
    "# Set GA01804 and MO20537 to IsLock = 0 because are not locks but have lock in name\n",
    "NID_join.loc[NID_join['NID'] == 'GA01804', 'IsLock'] = 0\n",
    "NID_join.loc[NID_join['NID'] == 'MO20537', 'IsLock'] = 0\n",
    "\n",
    "# Fill null values with 0\n",
    "NID_join.IsLock = NID_join.IsLock.fillna(0)\n",
    "\n",
    "# Drop columns we no longer need.\n",
    "NID_join = NID_join.drop(['NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','NID_ID','Moved','Deleted','lat','long'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9e060",
   "metadata": {},
   "source": [
    "#### Filter dams by name to remove any dam names that contain Spillway, Levee, Sewage, Treatment, Auxiliary, or Remedial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Size before filtering:', NID_join.shape)\n",
    "\n",
    "# Filter by name: Filter the dams that are NOT sites\n",
    "filters = \"Spillway|Levee|Sewage|Treatment|Auxiliary|Remedial\"\n",
    "##### AGAIN, IS THERE A CHANCE THAT ANY OF THESE NAMES ARE LOWERCASE BY ACCIDENT? if so, filters should be:\n",
    "##### filters = \"Spillway|Levee|Sewage|Treatment|Auxiliary|Remedial|spillway|levee|sewage|treatment|auxiliary|remedial\"\n",
    "\n",
    "# Only filter non-sites\n",
    "NIDs_filtered = NID_join.drop(index=NID_join.loc[NID_join.Dam_Name.str.contains(filters)==True].loc[(NID_join.IsSite==0)].loc[(NID_join.IsUSBR == 0)].loc[(NID_join.IsUSACE == 0)].index)\n",
    "\n",
    "NID_join = NIDs_filtered\n",
    "\n",
    "print('Size after filtering by name:', NIDs_filtered.shape) # New database size after filtering by name\n",
    "print('Number of sites in database:', NID_join.loc[NID_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7e560",
   "metadata": {},
   "source": [
    "#### Join remaining supplementary files and modify latitude and longitude appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18036348",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join to GRanD dams\n",
    "\n",
    "# Filter so only dams in the United States\n",
    "GRanD = GRanD[GRanD['COUNTRY'].str.contains('United States')== True]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Alaska') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Hawaii') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Puerto Rico') == False]\n",
    "                \n",
    "# Filter out columns we don't want\n",
    "GRanD = GRanD.drop(columns = ['RIVER','ALT_RIVER','MAIN_BASIN','SUB_BASIN','NEAR_CITY','ALT_CITY','SEC_ADMIN','COUNTRY','SEC_CNTRY','ALT_YEAR','ALT_HGT_M','DAM_LEN_M','ALT_LEN_M','AREA_SKM','AREA_POLY','AREA_REP','AREA_MAX','AREA_MIN','CAP_MAX','CAP_REP','CAP_MIN','DEPTH_M','DIS_AVG_LS','DOR_PC','ELEV_MASL','CATCH_SKM','CATCH_REP','DATA_INFO','USE_IRRI','USE_ELEC','USE_SUPP','USE_FCON','USE_RECR','USE_NAVI','USE_FISH','USE_PCON','USE_LIVE','USE_OTHR','MAIN_USE','LAKE_CTRL','MULTI_DAMS','TIMELINE','COMMENTS','URL','QUALITY','EDITOR','POLY_SRC'])\n",
    "\n",
    "print('Size of GRanD:', GRanD.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870da7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matches between GRanD NID and NID NID; outer join should keep GRanD dams that weren't in NID\n",
    "GRanD_join = pd.merge(NID_join, GRanD, on='NID', how='outer', suffixes = ('_NID','_GRanD'))\n",
    "\n",
    "# Combine output columns that were split in the join\n",
    "GRanD_join.loc[GRanD_join.ShortID_GRanD.notnull(), 'ShortID'] = GRanD_join.ShortID_GRanD\n",
    "GRanD_join.loc[GRanD_join.ShortID.isnull(), 'ShortID'] = GRanD_join.ShortID_NID\n",
    "\n",
    "# Assign 0 to NaNs\n",
    "GRanD_join.loc[GRanD_join.IsSite_NID.isna(),'IsSite_NID'] = 0\n",
    "GRanD_join['IsSite'] = GRanD_join['IsSite_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSBR_NID.isna(),'IsUSBR_NID'] = 0\n",
    "GRanD_join['IsUSBR'] = GRanD_join['IsUSBR_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSACE_NID.isna(),'IsUSACE_NID'] = 0\n",
    "GRanD_join['IsUSACE'] = GRanD_join['IsUSACE_NID']\n",
    "\n",
    "# Create a GRanD lat/long field that takes the GRanD lat/long preferentially. These fields are called NewX and NewY for our manual placements.\n",
    "GRanD_join['LAT_GRAND'] = GRanD_join['NewY']\n",
    "GRanD_join['LONG_GRAND'] = GRanD_join['NewX']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_GRAND = GRanD_join.LAT_GRAND.fillna(-999)\n",
    "GRanD_join.LONG_GRAND = GRanD_join.LONG_GRAND.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for sites; This preferentially chooses the site location over the GRanD location\n",
    "# If want to keep GRanD lat/long, skip this step\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LAT_GRAND'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LONG_GRAND'] = -999\n",
    "\n",
    "# Create a new field for lat/long that takes the GRanD lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join['LAT_JOIN'] = GRanD_join.LAT_GRAND\n",
    "GRanD_join['LONG_JOIN'] = GRanD_join.LONG_GRAND\n",
    "GRanD_join.loc[GRanD_join.LAT_GRAND == -999, 'LAT_JOIN'] = GRanD_join.Latitude\n",
    "GRanD_join.loc[GRanD_join.LONG_GRAND == -999, 'LONG_JOIN'] = GRanD_join.Longitude\n",
    "\n",
    "# Create a column for capacity from GRanD\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM * (1e6) # Convert million cubic meters to cubic m\n",
    "\n",
    "print('Size before joining to GRanD:',NID_join.shape)\n",
    "print('Size after joining to GRanD:',GRanD_join.shape) # New database size after adding GRanD dams\n",
    "print('Number of sites in database:',GRanD_join.loc[GRanD_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a4b546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matches between Removed Dams and NID NID; outer join should keep Removed dams that weren't in NID. \n",
    "# If you do not want to use a removed dams file, skip this step.\n",
    "GRanD_join = pd.merge(GRanD_join, removed, on='NID', how='outer', suffixes = ('_join','_rem'))\n",
    "\n",
    "# Create a GRanD lat/long field that takes the maximum lat/long out of the two joins\n",
    "##### ADD EXPLANATION FOR WHY TAKING MAXIMUM LAT/LONG, OR IF THIS NOT THE CASE THEN REPHRASE\n",
    "GRanD_join['LAT_Rem'] = GRanD_join['DamLatitud']\n",
    "GRanD_join['LONG_Rem'] = GRanD_join['DamLongitu']\n",
    "\n",
    "# Set null values to -999\n",
    "GRanD_join.LAT_Rem = GRanD_join.LAT_Rem.fillna(-999)\n",
    "GRanD_join.LONG_Rem = GRanD_join.LONG_Rem.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for sites, GRanD, USBR, and USACE. This preferentially keeps their locations.\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LONG_Rem'] = -999\n",
    "\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LONG_Rem'] = -999                                                                                    \n",
    "                                                                                    \n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'LONG_Rem'] = -999\n",
    "\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'LONG_Rem'] = -999\n",
    "                                                                                    \n",
    "# Create a new field for lat/long that takes the Removed dam lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join.loc[GRanD_join.LAT_Rem != -999, 'LAT_JOIN'] = GRanD_join.LAT_Rem\n",
    "GRanD_join.loc[GRanD_join.LONG_Rem != -999, 'LONG_JOIN'] = GRanD_join.LONG_Rem\n",
    "\n",
    "# Fix duplicate fields from join\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem.isnull() == True,'Batch_for_rem'] = -999 # Set null values to -999\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem != -999, 'Batch_for'] = GRanD_join.Batch_for_rem\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem == -999, 'Batch_for'] = GRanD_join.Batch_for_join\n",
    "\n",
    "print('Number of dams in database after joining to removed dams:',GRanD_join.shape) # New database size after adding removed dams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join to GeoDAR and add GeoDAR locations where able\n",
    "NID_geoDAR = pd.merge(GRanD_join, geoDAR[['NID','NewX','NewY']], on='NID', how='left')\n",
    "\n",
    "# Create a GRanD lat/long field that takes the maximum lat/long out of the two joins\n",
    "##### ADD EXPLANATION FOR WHY TAKING MAXIMUM LAT/LONG, OR IF THIS NOT THE CASE THEN REPHRASE\n",
    "NID_geoDAR['LAT_Geo'] = NID_geoDAR['NewY_y']\n",
    "NID_geoDAR['LONG_Geo'] = NID_geoDAR['NewX_y']\n",
    "\n",
    "# Set null values to -999\n",
    "NID_geoDAR.LAT_Geo = NID_geoDAR.LAT_Geo.fillna(-999)\n",
    "NID_geoDAR.LONG_Geo = NID_geoDAR.LONG_Geo.fillna(-999)\n",
    "\n",
    "# Make GRanD lat/long null for sites, GRanD, USACE, and USBR. This preferentially keeps their locations\n",
    "NID_geoDAR.loc[NID_geoDAR.IsSite == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsSite == 1, 'LONG_Geo'] = -999\n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LONG_Geo'] = -999                                                                                    \n",
    "                                                                                    \n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSBR == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSBR == 1, 'LONG_Geo'] = -999\n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSACE == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSACE == 1, 'LONG_Geo'] = -999\n",
    "                                                                                    \n",
    "# Create a new field for lat/long that takes the GeoDAR lat/long if not null and takes the original lat/long if null\n",
    "NID_geoDAR.loc[NID_geoDAR.LAT_Rem != -999, 'LAT_JOIN'] = NID_geoDAR.LAT_Geo\n",
    "NID_geoDAR.loc[NID_geoDAR.LONG_Rem != -999, 'LONG_JOIN'] = NID_geoDAR.LONG_Geo\n",
    "\n",
    "print('Number of dams in database:', NID_geoDAR.shape)\n",
    "\n",
    "GRanD_join = NID_geoDAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c69b861",
   "metadata": {},
   "source": [
    "#### Filter based on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6603f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign 0 to IsGRanD non-GRanD dams\n",
    "GRanD_join.IsGRanD = GRanD_join.IsGRanD.fillna(0)\n",
    "\n",
    "# Filter based on storage\n",
    "print('Size before storage filtering:', GRanD_join.shape)\n",
    "\n",
    "# Create column that takes the maximum of all of the storage values. This is NID_Storag in the NID table.\n",
    "GRanD_join['MaxStor_m3'] = GRanD_join['NID_Storag'] * 1233.48 # First convert AF to m^3\n",
    "\n",
    "# Fill MaxStor nans with 0\n",
    "GRanD_join.MaxStor_m3 = GRanD_join.MaxStor_m3.fillna(0)\n",
    "\n",
    "# Set GRanD_join storage sources to initially be NID and reference year to initially be Year_Compl\n",
    "# The outcome of this is that for each storage value reported, you have a source and a year that storage value represents\n",
    "GRanD_join['StorSource'] = 'NID'\n",
    "GRanD_join['Stor_Refyr'] = GRanD_join.Year_Compl\n",
    "\n",
    "# Replace any with MaxStor == 0 with GRanD storage, site storage, then USBR/USACE storage, then removed dams\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.YEAR\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.GRanDCapm3 # GRanD\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.Year_First\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.CapOrigAF*1233.48 # site; convert to AF to m^3\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.yr_p\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.Capm3_p # Reclamation/USACE\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.yrc\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_AF'] = GRanD_join.OrigCap_m3 # Removed dams\n",
    "\n",
    "# Any dams with maximum storage = 0 that is not a site or federal is removed\n",
    "GRanD_join = GRanD_join.drop(index=GRanD_join.loc[GRanD_join.MaxStor_m3 == 0].loc[(GRanD_join.IsSite==0) & (GRanD_join.IsUSBR == 0) & (GRanD_join.IsUSACE==0)].index)\n",
    "\n",
    "print('Size after storage filtering:',GRanD_join.shape) # New database size after filtering for storage\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d97e93",
   "metadata": {},
   "source": [
    "#### Assign ShortIDs to any dam that doesn't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33b848",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign unique ShortID to non-sites\n",
    "\n",
    "# First, sort by ascending NID\n",
    "GRanD_join = GRanD_join.sort_values('NID', ascending = True)\n",
    "\n",
    "# Starting ShortID should be the maximum of the site/GRanD ShortIDs plus 1,000 and rounded to the nearest thousandth\n",
    "startID = math.floor((GRanD_join.ShortID.max() + 1000)/1000)*1000\n",
    "ID = startID\n",
    "\n",
    "# Assign a ShortID to anything that doesn't have one yet       \n",
    "for index, row in GRanD_join.iterrows():\n",
    "    if pd.isna(row['ShortID']):  # Check if ShortID is null\n",
    "        GRanD_join.loc[index, 'ShortID'] = ID\n",
    "        ID += 1  # Increment ID for next ShortID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate ShortIDs. If there are duplicates, you have an error in your site input files. This is because either\n",
    "# you assigned two dams with different NIDs the same ShortID or could have a typo in the NID field.\n",
    "\n",
    "test = GRanD_join.ShortID\n",
    "\n",
    "nodup = set(test)\n",
    "\n",
    "if len(nodup) != len(test):\n",
    "    print('There are duplicate ShortIDs!')\n",
    "\n",
    "    newlist = [] # Empty list to hold unique elements from the list.\n",
    "    duplist = [] # Empty list to hold the duplicate elements from the list.\n",
    "    for i in test:\n",
    "        if i not in newlist:\n",
    "            newlist.append(i)\n",
    "        else:\n",
    "            duplist.append(i) # This method catches the first duplicate entries, and appends them to the list.\n",
    "            \n",
    "    # The next step is to print the duplicate entries, and the unique entries\n",
    "    print(\"List of duplicates\", duplist)\n",
    "else:\n",
    "    print('There are no duplicate ShortIDs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cc6447",
   "metadata": {},
   "source": [
    "#### Clean up columns and export as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63b98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine columns where necessary and drop unwanted columns\n",
    "\n",
    "# Combine NID Dam height and removed dam heights into one field. Only put in removed dam where NID is null\n",
    "GRanD_join['DamH_ft'] = GRanD_join.NID_Height\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull() == True),'DamH_ft'] = GRanD_join.DAmHft\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.DAM_HGT_M*3.28\n",
    "\n",
    "# Fill in Dam function from removed dam file everywhere PrimPurp is null\n",
    "# Then replace the null PrimaryPurp with Purp because some have null PrimaryPurp and non-null Purp.\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.DamFunctio\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.Purp\n",
    "\n",
    "# Year completed\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull()==True, 'yrc'] = GRanD_join.Year_Compl # Anywhere removed dam database is null, change to NID\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull()==True, 'yrc'] = GRanD_join.YEAR # Anything else still null fill with GRanD\n",
    "\n",
    "# Year removed\n",
    "GRanD_join.loc[GRanD_join.yrr.isnull()==True, 'yrr'] = GRanD_join.YrRemoved #site/USACE/USBR\n",
    "GRanD_join.loc[GRanD_join.yrr.isnull()==True, 'yrr'] = GRanD_join.REM_YEAR #GRanD\n",
    "\n",
    "# Dam name\n",
    "GRanD_join.loc[GRanD_join.Dam_Name=='','Dam_Name'] = np.nan # Set values we made blank earlier back to nan\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Other_Dam # Replace missing NID names with NID other name first\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Reservoir # Then site\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.USBRname # Then USBR\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.RES_NAME # Then GRanD\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.DamName # Then Removed file\n",
    "\n",
    "# Drop unwanted columns at this point\n",
    "GRanD_join = GRanD_join.drop(['index','OtherStructureID','FederalID','Longitude','Latitude','River','Owner_Name',\n",
    "                             'Max_Storag','Normal_Sto','Lat','Long','Batch_for_join','ShortID_NID',\n",
    "                              'IsSite_NID','IsUSBR_NID','IsUSACE_NID','DAM_NAME','ALT_NAME',\n",
    "                              'ADMIN_UNIT','LONG_DD','LAT_DD','NewX_x','NewY_x','NIDnotes','ShortID_GRanD','HasNHD',\n",
    "                              'IsSite_GRanD','IsUSBR_GRanD','IsUSACE_GRanD','Field','LAT_GRAND','LONG_GRAND',\n",
    "                              'CitationID','CitationUR','DamAccessi','DamRiverNa','DamRiver_1','DamLocatio',\n",
    "                              'DamState_P','DamLatitud','DamLongitu','DamAccurac','DamOwner','Batch_for_rem',\n",
    "                              'LAT_Rem','LONG_Rem','NewX_y','NewY_y','LAT_Geo','LONG_Geo','NID_Height','DamH_m',\n",
    "                              'DAmHft','Purp','DamFunctio','SiteIsGRanD','GRanD_ID','Other_Dam','REM_YEAR','RES_NAME',\n",
    "                              'DamName','DAM_HGT_M','USACE_PROJECT_ID','YEAR'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to metric and drop the ft column\n",
    "GRanD_join['Dam_Len_m'] = GRanD_join.Dam_Length*0.3048\n",
    "GRanD_join['SA_m2'] = GRanD_join.Surface_Ar*4046.85642\n",
    "GRanD_join['DA_km2'] = GRanD_join.Drainage_A*2.58998811\n",
    "GRanD_join['MaxQ_m3s'] = GRanD_join.Max_Discha*0.028316847\n",
    "GRanD_join['DamH_m'] = GRanD_join.DamH_ft * 0.3048\n",
    "GRanD_join['NIDStor_m3'] = GRanD_join.NID_Storag*1233.48185532\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM*(1e6)\n",
    "GRanD_join['elev_m'] = GRanD_join.elev_ft*0.3048\n",
    "\n",
    "# Drop columns with imperial units\n",
    "GRanD_join = GRanD_join.drop(['Dam_Length','Surface_Ar','Drainage_A','Max_Discha','CapOrigAF','CapNewAF','CapAF_p',\n",
    "                             'DamNameAlt','NID_Storag','CAP_MCM','elev_ft','DamH_ft',\n",
    "                              ],axis=1)\n",
    "# Rename columns\n",
    "GRanD_join.rename(columns = {'OrigCap_m3':'OCapm3_Rem','LAT_JOIN':'LAT_FINAL','LONG_JOIN':'LONG_FINAL',\n",
    "                             'DA_km':'site_DA_km'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e506bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final files as a csv\n",
    "GRanD.to_csv(os.path.join(out_folder,'GRanD.csv'))\n",
    "GRanD_join.to_csv(os.path.join(out_folder,'NID_GRanDjoin.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b74d07",
   "metadata": {},
   "source": [
    "# 2. Snap dams to NHDPlus Flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e590628",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Snap dams to NHDPlus HR Flowlines: must be done with arcPy. Ensure that all layers are in the same coordinate system (here we use NAD83).\n",
    "\n",
    "# First convert the csv to a shapefile\n",
    "XFieldName = 'LONG_FINAL'\n",
    "YFieldName = 'LAT_FINAL'\n",
    "newLayerName = \"NID_filtered\" # Name of your output shapefile\n",
    "\n",
    "spatialRef = arcpy.SpatialReference(4269) # Spatial reference WKID for NAD83\n",
    "csvFilePath = os.path.join(out_folder,'NID_GRanDjoin.csv') # Your filtered dam dataset csv\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, newLayerName, spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion(newLayerName, out_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6c7a94-123d-41c8-8aaa-9846afb7ab18",
   "metadata": {},
   "source": [
    "*The following 2 cells will take multiple hours to run, so plan accordingly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd104d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For NHDPlus\n",
    "NIDFiltered = os.path.join(out_folder,'NID_filtered.shp') #Link to your filtered NID shapefile\n",
    "\n",
    "NIDlyr = \"NIDlyr\" #create a layer file\n",
    "NHDlyr = \"NHDlyr\"\n",
    "arcpy.management.MakeFeatureLayer(NIDFiltered,NIDlyr) #convert the feature class to a layer to work from\n",
    "arcpy.management.MakeFeatureLayer(NHDFlowline,NHDlyr)\n",
    "\n",
    "#Run the near tool to get the new lat/long with near FType558\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE = 55800\")\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 0\") #select all non-sites, non-Reclamation, non-USACE, and non-GRanD to snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\")\n",
    "\n",
    "#Near 250 m to FType558 for everything that isn't a site\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Create a new field and populate it\n",
    "arcpy.management.AddField(NIDlyr,'NrX_Final',\"DOUBLE\")\n",
    "arcpy.management.AddField(NIDlyr,'NrY_Final',\"DOUBLE\")\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline FType 558 Selection before transferring over values\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Near 500m to all flowlines for any that didn't snap and have MaxStor >= 4000 AF/5,0000,000 m^3\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And not GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3>=5e6\") #MaxStor >= 4000 AF\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"500 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Near 250m to all flowlines for any that didn't snap and still aren't a site\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And any that aren't GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Sites to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 1\") #Select all sites\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#GRanD to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsGRanD = 1\") #Select all GRanD\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#USBR to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSBR = 1\") #Select all USBR\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#USACE to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSACE = 1\") #Select all USACE\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#Delete any features that did not snap.\n",
    "#First select non-snaps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION')\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\")\n",
    "\n",
    "#Delete the selected rows:\n",
    "arcpy.management.DeleteRows(NIDlyr)\n",
    "arcpy.GetCount_management(NIDlyr)\n",
    "\n",
    "#Display XY data using NearX and NearY to move the points onto the NHD Flowlines\n",
    "arcpy.management.MakeXYEventLayer(NIDlyr, \"NrX_Final\", \"NrY_Final\", 'NIDFiltered_snap', spatialRef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9885d771",
   "metadata": {},
   "source": [
    "### Intersect with the flowline data to extract attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de94ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersect file with moved dams to the flowline file:\n",
    "arcpy.analysis.Intersect(['NIDFiltered_snap',NHDFlowline],os.path.join(out_folder,'NIDFiltered_snap.shp'),\"ALL\",None,\"INPUT\")\n",
    "\n",
    "#Export NID file\n",
    "dbf = Dbf5(os.path.join(out_folder,'NIDFiltered_snap.dbf'))\n",
    "df = dbf.to_dataframe()\n",
    "df.to_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4775fbd",
   "metadata": {},
   "source": [
    "### Remove duplicate flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate flowlines. When you run intersect in ArcGIS, any intersects that happen at the join of two lines gives two\n",
    "# results in the final table. We need to delete one of these.\n",
    "\n",
    "NID = pd.read_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'), low_memory=False) # Load data.\n",
    "NID_sort = NID.sort_values('ShortID', ascending = True) # Sort by ascending ShortID.\n",
    "\n",
    "print('Size before duplicates removed:', NID_sort.shape)\n",
    "\n",
    "# Convert dataframe to dictionaries (struct-like); basically has format column->value.\n",
    "NID_dict = NID_sort.to_dict(orient = 'records')\n",
    "dupl_ordered_dict = NID.to_dict(orient='records')\n",
    "\n",
    "# Initialize empty list to store indices of non-duplicates.\n",
    "dupind = []\n",
    "\n",
    "# Identify unique values and their counts.\n",
    "shortID = [item['ShortID'] for item in dupl_ordered_dict]\n",
    "uniquevals,ia = np.unique(shortID, return_inverse = True)\n",
    "\n",
    "# Count the frequency of each index in ia.\n",
    "bincounts = np.bincount(ia)\n",
    "\n",
    "# Zero out singles.\n",
    "singles = uniquevals[bincounts <= 1]\n",
    "singleidx = [i for i, val in enumerate(shortID) if val in singles]\n",
    "for idx in singleidx:\n",
    "    shortID[idx] = 0\n",
    "    \n",
    "# Overwrite repeats.\n",
    "repeats = uniquevals[bincounts > 1]\n",
    "shortID = np.array([np.where(repeats == val)[0][0] + 1 if val in repeats else val for val in shortID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_it = 0; # Initialize a counter.\n",
    "# Pull out FCODE and Hydrosequence fields to help with decision tree for removing duplicates.\n",
    "FCODE = [item['FCODE'] for item in dupl_ordered_dict]\n",
    "Hydroseq = [item['Hydroseq'] for item in dupl_ordered_dict]\n",
    "\n",
    "\n",
    "for i in range(len(shortID)):\n",
    "    if shortID[i] == 0: # If it is not a duplicate, keep it.\n",
    "        dupind.append(i)\n",
    "    elif skip_it > 0: # Or if we already dealt with it, update the counter so it gets skipped.\n",
    "        skip_it -= 1\n",
    "        continue\n",
    "    else: # Else the value is a duplicate.\n",
    "        dup = [idx for idx, val in enumerate(shortID) if val == shortID[i]] # Gives all indices of the duplicates.\n",
    "        dup1 = dup[0]\n",
    "        dupskip = dup1 # Keep track of what the first index was because we will change this.\n",
    "        j = len(dup)\n",
    "        jskip = j # Same for the length of the duplicates.\n",
    "\n",
    "        Hydro = Hydroseq[dup1:dup[j-1]+1] # Pull out Hydrosequences as the duplicates.\n",
    "\n",
    "        kept_indices = [i for i, x in enumerate(Hydro) if x not in [Hydroseq[i] for i in dupind]] # If a dam is already snapped to that flowline, remove the flowlines from the options to choose from.\n",
    "\n",
    "        dup_test = [dup[i] for i in kept_indices]\n",
    "                \n",
    "        if len(dup_test) == 0: # All of the flowline options have already been used, in which case just keep them all. Duplicate snaps are removed later.\n",
    "            dup_test = dup\n",
    "        \n",
    "        dup = dup_test\n",
    "        \n",
    "        dup1 = dup[0]\n",
    "        j = len(dup)\n",
    "        \n",
    "        Floc = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 55800] # Pull out FCODE = 55800 for duplicates (flowlines in reservoirs).\n",
    "        coast = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 56600] # Pull out any duplicates that are on a coast flowline.\n",
    "        \n",
    "        \n",
    "    \n",
    "        if len(Floc) == 1: # If only one value is FType 558.\n",
    "            dupind.append(dup[Floc[0]])\n",
    "        elif len(Floc) == j: # All of the values are 558, take smallest hydroseq (most downstream).\n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[minloc])\n",
    "        elif len(Floc) == 0: # None are FType 558.         \n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "           \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast) # Remove coastal values; any dam that snaps to a coast flowline has it's dam order messed up and can route along the coast.\n",
    "                dup = np.delete(dup,coast)\n",
    "\n",
    "            minloc = np.argmin(Hydro) # Currently taking minimum of the new hydro.\n",
    "            dupind.append(dup[minloc])\n",
    "        else: # Some other number of values is FType 558; still take the most downstream.\n",
    "            Hydro = [Hydroseq[dup[index]] for index in Floc]\n",
    "    \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast)\n",
    "                dup = np.delete(dup,coast)\n",
    "                \n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[Floc[minloc]])\n",
    "            \n",
    "        if dupskip == i: # If the first index was the current index.\n",
    "            skip_it = jskip-1 # Skip the next j-1 indices.\n",
    "        else:\n",
    "            skip_it = 0\n",
    "\n",
    "dupltable = pd.DataFrame.from_dict(dupl_ordered_dict)\n",
    "noduplicates = dupltable.loc[dupind]\n",
    "\n",
    "noduplicates.to_csv(os.path.join(out_folder,'NID_filtered_snapped_nodupl.csv'),index = False)\n",
    "\n",
    "\n",
    "print('Size after removing duplicates:',noduplicates.shape) # New database size after snapping to NHD flowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a7ad0-fff3-47ca-957c-6be6c9c21718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
