{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351ac3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written 8/11/2022 to automate the filtering process and assign dam order to all dams in the nation.\n",
    "# The starting file for this script is a csv file of the NID database dams for the entire nation \n",
    "# downloaded from https://nid.sec.usace.army.mil/#/downloads. You should be able to run this for only a subset\n",
    "# of the dams as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414966a",
   "metadata": {},
   "source": [
    "# 1. Import databases and filter them. Combine databases as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1237f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTables is not installed. No support for HDF output.\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import math\n",
    "from simpledbf import Dbf5\n",
    "import os\n",
    "\n",
    "#import arcpy packages\n",
    "import archook\n",
    "arcgis_python_path = r'C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3' #The path on your computer where your ArcGIS Python installation is located\n",
    "\n",
    "# Add the Python environment to the path\n",
    "archook.arcpy = arcgis_python_path\n",
    "\n",
    "# archook.get_arcpy()\n",
    "import arcpy\n",
    "\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fffc57",
   "metadata": {},
   "source": [
    "#### Load data here. You will need to replace all of the input files with the locations of your copies of the file. The output_folder variable is the folder where all of the output files will be saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0caf45f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_33772\\1026226165.py:4: DtypeWarning: Columns (4,41,57,78,79) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  NIDs = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID2024.csv',header=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of NID in database: (91824, 84)\n",
      "Number of USBR in database: (201, 32)\n",
      "Number of USACE in database: (345, 32)\n",
      "Number of sites in database: (1053, 32)\n"
     ]
    }
   ],
   "source": [
    "## load data. See the files included with the report for formatting.\n",
    "\n",
    "# load NID data.\n",
    "NIDs = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID2024.csv',header=1)\n",
    "\n",
    "#load removed dams file if using.\n",
    "removed = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/RemovedDams.csv')\n",
    "\n",
    "#load spatial edits file if manually have changed any locations or deleted dams from NID.\n",
    "editNID = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NID_spatialEdits.csv') #cross reference with deleting S001 etc.\n",
    "\n",
    "#load GeoDAR data to use as location where no NID location.\n",
    "geoDAR = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GeoDAR_crossref.csv')\n",
    "\n",
    "#load Federal dam and site combined data for if you have sites with additional data that you want included. These dams\n",
    "#have been manually placed on flowlines and have accurate locations.\n",
    "sites = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/sites.csv')\n",
    "\n",
    "#NHD Plus Medium Resolution flowline shapefile\n",
    "NHDFlowline = 'D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NHDFlowline_Network_NHDPlus_Countries.gdb/NHDFlowline_Network_NHDPlus_Countries'  #path for NHD flowline shapefile\n",
    "\n",
    "# load GRanD data. We are using v1.3 with some modifications to locations that places GRanD on NHD Flowlines.\n",
    "GRanD = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/GRanD_dams_v1_3.csv')\n",
    "\n",
    "#D50 Data\n",
    "D50 = pd.read_csv('D:/ResSed/MediumResolution_DamLinkages/Manuscript/FinalInputFiles/NHDPlus_MediumResolution_D50.csv',header=0)\n",
    "\n",
    "#output folder directory where you want to save the output files.\n",
    "out_folder = 'D:/ResSed/MediumResolution_DamLinkages/Manuscript/Outputs'\n",
    "\n",
    "\n",
    "#Print original numbers of dams in various databases for tracking:\n",
    "print('Original number of NID in database:',NIDs.shape)\n",
    "print('Number of USBR in database:',sites.loc[sites.IsUSBR==1].shape)\n",
    "print('Number of USACE in database:',sites.loc[sites.IsUSACE==1].shape)\n",
    "print('Number of sites in database:',sites.loc[sites.IsSite==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801d34a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of raw NIDs with no locations: (2, 84)\n"
     ]
    }
   ],
   "source": [
    "#Count how many dams have lat/long as 0.\n",
    "noLoc = NIDs[(NIDs.Latitude == 0) | (NIDs.Longitude == 0)]\n",
    "\n",
    "# noLoc.head()\n",
    "print('Number of raw NIDs with no locations:', noLoc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "723169f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean NID data to only keep the columns that we need\n",
    "#FOR 2024 NID FILE. Column headers may change between versions of NID downloads. Check this if re-running with a new NID download.\n",
    "\n",
    "#rename columns\n",
    "new_cols = ['Dam_Name','Other_Dam','Former_Name','NID','OtherStructureID','FederalID','Owner_Name','OwnerTypes','PrimaryOwnerType','NumStruct','AssStruct','Designer','NonFedDam','PrimaryPurp', 'Purp','SourceAgency','StateorFedID','Latitude','Longitude','State','County','City','DisttoCity','River','CongressDist','AmInd','SecLoc','StateReg','Juris','Agency','StatePerm','StateInsp','StateEnforce','FedReg','FedOwner','FedFunding','FedDesign','FedConst','FedReg','FedInsp','FedOps','FedOther','SecAg','NRCS','PrimDamType','DamTypes','CoreTypes','Foundation','Dam_Height','HydraulicHeight','StructHeight','NID_Height','NIDHeightCat','Dam_Length','Volume','Year_Compl','YearCompCat','Year_Modif','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','SpillwayType','SpillWidth','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','OutletGate','DataUpdated','LastInspection','InspectionFreq','HazardClass','CondAss','CondAssDate','OpStat','OpStatDate','EAPPrep','EAPRev','InundationMap','URL']\n",
    "NIDs.columns = new_cols\n",
    "\n",
    "\n",
    "# Filter and rename variables\n",
    "NIDs = NIDs[['Dam_Name','Other_Dam','NID','OtherStructureID','FederalID','Longitude','Latitude','State','River','Owner_Name','Year_Compl','Year_Modif','NID_Height','Dam_Length','NID_Storag','Max_Storag','Normal_Sto','Surface_Ar','Drainage_A','Max_Discha','PrimaryPurp','Purp','PrimDamType','NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea85568",
   "metadata": {},
   "source": [
    "#### Move and delete NID dams using the spatial edits file if so desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9013c7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length to delete: (91, 5)\n",
      "NID database size: (91737, 33)\n"
     ]
    }
   ],
   "source": [
    "#Use spatial edit file to change lat/long of moved dams and delete deleted dams. If you do not wish to modify original NID, skip this.\n",
    "delete = editNID.loc[editNID.Deleted == 1]\n",
    "move = editNID.loc[editNID.Moved == 1]\n",
    "\n",
    "NIDs = NIDs[~NIDs['NID'].isin(delete['NID_ID'])] #Drop delete NIDs\n",
    "\n",
    "NIDs = pd.merge(NIDs, move, left_on = 'NID', right_on = 'NID_ID', how = 'left')\n",
    "\n",
    "#Chane the Latitude and Longitude fields of the NIDs you wish to move.\n",
    "NIDs.loc[NIDs.Moved == 1,'Latitude'] = NIDs.lat\n",
    "NIDs.loc[NIDs.Moved == 1, 'Longitude'] = NIDs.long\n",
    "\n",
    "print('Length to delete:', delete.shape)\n",
    "print('NID database size:',NIDs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05caf221",
   "metadata": {},
   "source": [
    "#### Join in the site file that has sites with additional data if using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5886a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the site data to the NID database\n",
    "\n",
    "NID_join = pd.merge(NIDs, sites, on='NID', how='outer')\n",
    "\n",
    "# Replace Storage NaNs with 0\n",
    "NID_join.NID_Storag = NID_join.NID_Storag.fillna(0)\n",
    "NID_join.Max_Storag = NID_join.Max_Storag.fillna(0)\n",
    "NID_join.Normal_Sto = NID_join.Normal_Sto.fillna(0)\n",
    "NID_join.IsUSBR = NID_join.IsUSBR.fillna(0)\n",
    "NID_join.IsUSACE = NID_join.IsUSACE.fillna(0)\n",
    "NID_join.IsSite = NID_join.IsSite.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f4c66b",
   "metadata": {},
   "source": [
    "#### Remove duplicate NIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3fc919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size after removing duplicates and joining to site file: (90218, 64)\n",
      "Number of sites in database: (1053, 64)\n",
      "Number of Reclamation dams in database: (201, 64)\n",
      "Number of USACE dams in database: (345, 64)\n"
     ]
    }
   ],
   "source": [
    "## Remove duplicate NIDs and keep the largest reported storage data\n",
    "NIDs_join = NID_join.drop(index=NID_join.loc[NID_join.OtherStructureID.notnull()].index)\n",
    "\n",
    "\n",
    "#A couple don't have Other Structure ID but are duplicates. Filter those by storage. Should only be 2.\n",
    "# sort the data by descending max storage\n",
    "NID_join = NID_join.sort_values('NID_Storag', ascending = False)\n",
    "\n",
    "# Remove duplicate NIDs, keeping the first value aka the biggest capacity\n",
    "bool_series = NID_join['NID'].duplicated()\n",
    "NID_join = NID_join[~bool_series]\n",
    "\n",
    "\n",
    "#Print checks\n",
    "print('Size after removing duplicates and joining to site file:',NID_join.shape)\n",
    "print('Number of sites in database:',NID_join.loc[NID_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405dcac",
   "metadata": {},
   "source": [
    "#### Update latitude and longitude based on known lat/long from the site file and populate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b6c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NID_join = NID_join.reset_index()\n",
    "\n",
    "\n",
    "# Set lat and long for sites to be the site lat/long\n",
    "NID_join.loc[NID_join.IsSite == 1,'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsSite == 1, 'Longitude'] = NID_join.Long\n",
    "\n",
    "#Set lat and long for Reclamation dams\n",
    "NID_join.loc[NID_join.IsUSBR == 1, 'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsUSBR == 1, 'Longitude'] = NID_join.Long\n",
    "\n",
    "NID_join.loc[NID_join.IsUSACE == 1, 'Latitude'] = NID_join.Lat\n",
    "NID_join.loc[NID_join.IsUSACE == 1, 'Longitude'] = NID_join.Long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9a58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate IsLock column\n",
    "#replace null values with 0\n",
    "NID_join.NumLocks = NID_join.NumLocks.fillna(0)\n",
    "NID_join.LengthLocks = NID_join.LengthLocks.fillna(0)\n",
    "NID_join.LockWidth = NID_join.LockWidth.fillna(0)\n",
    "NID_join.LengthSecondLock = NID_join.LengthSecondLock.fillna(0)\n",
    "NID_join.SecondLockWidth = NID_join.SecondLockWidth.fillna(0)\n",
    "NID_join.loc[(NID_join.NumLocks>0) & (NID_join.NumLocks<10),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LengthLocks>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LockWidth>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.LengthSecondLock>0),'IsLock'] = 1\n",
    "NID_join.loc[(NID_join.SecondLockWidth>0),'IsLock'] = 1\n",
    "\n",
    "\n",
    "#Search by name containing word Lock\n",
    "NID_join['Dam_Name'] = NID_join['Dam_Name'].fillna('') #First fill NA names with empty strings\n",
    "NID_join.loc[NID_join['Dam_Name'].str.contains('Lock '), 'IsLock'] = 1\n",
    "\n",
    "#Set GA01804 and MO20537 to IsLock = 0 because are not locks but have lock in name\n",
    "NID_join.loc[NID_join['NID'] == 'GA01804', 'IsLock'] = 0\n",
    "NID_join.loc[NID_join['NID'] == 'MO20537', 'IsLock'] = 0\n",
    "\n",
    "#Fill nans with 0\n",
    "NID_join.IsLock = NID_join.IsLock.fillna(0)\n",
    "\n",
    "#drop columns we no longer need\n",
    "NID_join = NID_join.drop(['NumLocks','LengthLocks','LockWidth','LengthSecondLock','SecondLockWidth','NID_ID','Moved','Deleted','lat','long'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166527f",
   "metadata": {},
   "source": [
    "#### Filter dams by name to remove any dam names that contain Spillway, Levee, Sewage, Treatment, Auxiliary, or Remedial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4912f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before filtering: (90218, 55)\n",
      "Size after filtering by name: (89734, 55)\n",
      "Number of sites in database: (1053, 55)\n",
      "Number of Reclamation dams in database: (201, 55)\n",
      "Number of USACE dams in database: (345, 55)\n"
     ]
    }
   ],
   "source": [
    "print('Size before filtering:', NID_join.shape)\n",
    "\n",
    "## Filter by name: Filter the dams that are NOT sites\n",
    "filters = \"Spillway|Levee|Sewage|Treatment|Auxiliary|Remedial\"\n",
    "\n",
    "#Only filter non-sites\n",
    "NIDs_filtered = NID_join.drop(index=NID_join.loc[NID_join.Dam_Name.str.contains(filters)==True].loc[(NID_join.IsSite==0)].loc[(NID_join.IsUSBR == 0)].loc[(NID_join.IsUSACE == 0)].index)\n",
    "\n",
    "NID_join = NIDs_filtered\n",
    "\n",
    "print('Size after filtering by name:',NIDs_filtered.shape)\n",
    "print('Number of sites in database:',NID_join.loc[NID_join.IsSite == 1].shape) #this should be 535\n",
    "print('Number of Reclamation dams in database:', NID_join.loc[NID_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', NID_join.loc[NID_join.IsUSACE == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa61b1",
   "metadata": {},
   "source": [
    "#### Join remaining supplementary files and modify latitude and longitude appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94493eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of GRanD: (1897, 22)\n"
     ]
    }
   ],
   "source": [
    "## Join to GRanD dams\n",
    "\n",
    "#Filter so only dams in the United States\n",
    "GRanD = GRanD[GRanD['COUNTRY'].str.contains('United States')== True]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Alaska') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Hawaii') == False]\n",
    "GRanD = GRanD[GRanD['ADMIN_UNIT'].str.contains('Puerto Rico') == False]\n",
    "                \n",
    "#Filter out columns don't want\n",
    "GRanD = GRanD.drop(columns = ['RIVER','ALT_RIVER','MAIN_BASIN','SUB_BASIN','NEAR_CITY','ALT_CITY','SEC_ADMIN','COUNTRY','SEC_CNTRY','ALT_YEAR','ALT_HGT_M','DAM_LEN_M','ALT_LEN_M','AREA_SKM','AREA_POLY','AREA_REP','AREA_MAX','AREA_MIN','CAP_MAX','CAP_REP','CAP_MIN','DEPTH_M','DIS_AVG_LS','DOR_PC','ELEV_MASL','CATCH_SKM','CATCH_REP','DATA_INFO','USE_IRRI','USE_ELEC','USE_SUPP','USE_FCON','USE_RECR','USE_NAVI','USE_FISH','USE_PCON','USE_LIVE','USE_OTHR','MAIN_USE','LAKE_CTRL','MULTI_DAMS','TIMELINE','COMMENTS','URL','QUALITY','EDITOR','POLY_SRC'])\n",
    "\n",
    "print('Size of GRanD:', GRanD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9c86628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before joining to GRanD: (89734, 55)\n",
      "Size after joining to GRanD: (89803, 85)\n",
      "Number of sites in database: (1053, 85)\n",
      "Number of Reclamation dams in database: (201, 85)\n",
      "Number of USACE dams in database: (345, 85)\n",
      "Number of GRanD in database: (1897, 85)\n"
     ]
    }
   ],
   "source": [
    "#find matches between GRanD NID and NID NID; outer join should keep GRanD dams that weren't in NID\n",
    "GRanD_join = pd.merge(NID_join, GRanD, on='NID', how='outer', suffixes = ('_NID','_GRanD'))\n",
    "\n",
    "#combine output columns that were split in the join\n",
    "GRanD_join.loc[GRanD_join.ShortID_GRanD.notnull(), 'ShortID'] = GRanD_join.ShortID_GRanD\n",
    "GRanD_join.loc[GRanD_join.ShortID.isnull(), 'ShortID'] = GRanD_join.ShortID_NID\n",
    "\n",
    "GRanD_join.loc[GRanD_join.IsSite_NID.isna(),'IsSite_NID'] = 0\n",
    "GRanD_join['IsSite'] = GRanD_join['IsSite_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSBR_NID.isna(),'IsUSBR_NID'] = 0\n",
    "GRanD_join['IsUSBR'] = GRanD_join['IsUSBR_NID']\n",
    "GRanD_join.loc[GRanD_join.IsUSACE_NID.isna(),'IsUSACE_NID'] = 0\n",
    "GRanD_join['IsUSACE'] = GRanD_join['IsUSACE_NID']\n",
    "\n",
    "#Create a GRanD lat/long field that takes the GRanD lat/long preferentially. These fields are called NewX and NewY for our manual placements\n",
    "GRanD_join['LAT_GRAND'] = GRanD_join['NewY']\n",
    "GRanD_join['LONG_GRAND'] = GRanD_join['NewX']\n",
    "\n",
    "#set null values to -999\n",
    "GRanD_join.LAT_GRAND = GRanD_join.LAT_GRAND.fillna(-999)\n",
    "GRanD_join.LONG_GRAND = GRanD_join.LONG_GRAND.fillna(-999)\n",
    "\n",
    "\n",
    "#Make GRanD lat/long null for sites; This preferentially chooses the site location over the GRanD location.\n",
    "#If want to keep GRanD lat/long, skip this step\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LAT_GRAND'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LONG_GRAND'] = -999\n",
    "\n",
    "\n",
    "#Create a new field for lat/long that takes the GRanD lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join['LAT_JOIN'] = GRanD_join.LAT_GRAND\n",
    "GRanD_join['LONG_JOIN'] = GRanD_join.LONG_GRAND\n",
    "GRanD_join.loc[GRanD_join.LAT_GRAND == -999, 'LAT_JOIN'] = GRanD_join.Latitude\n",
    "GRanD_join.loc[GRanD_join.LONG_GRAND == -999, 'LONG_JOIN'] = GRanD_join.Longitude\n",
    "\n",
    "# #Create a column for capacity from GRanD\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM * (1e6) #million cubic meters to cubic m\n",
    "\n",
    "print('Size before joining to GRanD:',NID_join.shape)\n",
    "print('Size after joining to GRanD:',GRanD_join.shape) #should have 42 more\n",
    "print('Number of sites in database:',GRanD_join.loc[GRanD_join.IsSite == 1].shape) \n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape) #this should be 1903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d50e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database after joining to removed dams: (89846, 109)\n"
     ]
    }
   ],
   "source": [
    "#find matches between Removed Dams and NID NID; outer join should keep Removed dams that weren't in NID. If you do not want to use a removed dams file, skip this step.\n",
    "GRanD_join = pd.merge(GRanD_join, removed, on='NID', how='outer', suffixes = ('_join','_rem'))\n",
    "\n",
    "\n",
    "# #Create a GRanD lat/long field that takes the maximum lat/long out of the two joins\n",
    "GRanD_join['LAT_Rem'] = GRanD_join['DamLatitud']\n",
    "GRanD_join['LONG_Rem'] = GRanD_join['DamLongitu']\n",
    "\n",
    "#set null values to -999\n",
    "GRanD_join.LAT_Rem = GRanD_join.LAT_Rem.fillna(-999)\n",
    "GRanD_join.LONG_Rem = GRanD_join.LONG_Rem.fillna(-999)\n",
    "\n",
    "#Make GRanD lat/long null for sites, GRanD, USBR, and USACE. This preferentially keeps their locations.\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'LONG_Rem'] = -999\n",
    "\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsGRanD == 1, 'LONG_Rem'] = -999                                                                                    \n",
    "                                                                                    \n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'LONG_Rem'] = -999\n",
    "\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'LAT_Rem'] = -999\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'LONG_Rem'] = -999\n",
    "                                                                                    \n",
    "#Create a new field for lat/long that takes the Removed dam lat/long if not null and takes the original lat/long if null\n",
    "GRanD_join.loc[GRanD_join.LAT_Rem != -999, 'LAT_JOIN'] = GRanD_join.LAT_Rem\n",
    "GRanD_join.loc[GRanD_join.LONG_Rem != -999, 'LONG_JOIN'] = GRanD_join.LONG_Rem\n",
    "\n",
    "\n",
    "# #Fix duplicate fields from join\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem.isnull() == True,'Batch_for_rem'] = -999 #set null values to -999\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem != -999, 'Batch_for'] = GRanD_join.Batch_for_rem\n",
    "GRanD_join.loc[GRanD_join.Batch_for_rem == -999, 'Batch_for'] = GRanD_join.Batch_for_join\n",
    "\n",
    "print('Number of dams in database after joining to removed dams:',GRanD_join.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea883049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dams in database: (89846, 113)\n"
     ]
    }
   ],
   "source": [
    "#join to GeoDAR and add GeoDAR locations where able\n",
    "NID_geoDAR = pd.merge(GRanD_join,geoDAR[['NID','NewX','NewY']],on='NID', how='left')\n",
    "\n",
    "# #Create a GRanD lat/long field that takes the maximum lat/long out of the two joins\n",
    "NID_geoDAR['LAT_Geo'] = NID_geoDAR['NewY_y']\n",
    "NID_geoDAR['LONG_Geo'] = NID_geoDAR['NewX_y']\n",
    "\n",
    "#set null values to -999\n",
    "NID_geoDAR.LAT_Geo = NID_geoDAR.LAT_Geo.fillna(-999)\n",
    "NID_geoDAR.LONG_Geo = NID_geoDAR.LONG_Geo.fillna(-999)\n",
    "\n",
    "#Make GRanD lat/long null for sites, GRanD, USACE, and USBR. This preferentially keeps their locations.\n",
    "NID_geoDAR.loc[NID_geoDAR.IsSite == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsSite == 1, 'LONG_Geo'] = -999\n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsGRanD == 1, 'LONG_Geo'] = -999                                                                                    \n",
    "                                                                                    \n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSBR == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSBR == 1, 'LONG_Geo'] = -999\n",
    "\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSACE == 1, 'LAT_Geo'] = -999\n",
    "NID_geoDAR.loc[NID_geoDAR.IsUSACE == 1, 'LONG_Geo'] = -999\n",
    "                                                                                    \n",
    "#Create a new field for lat/long that takes the GeoDAR lat/long if not null and takes the original lat/long if null\n",
    "NID_geoDAR.loc[NID_geoDAR.LAT_Rem != -999, 'LAT_JOIN'] = NID_geoDAR.LAT_Geo\n",
    "NID_geoDAR.loc[NID_geoDAR.LONG_Rem != -999, 'LONG_JOIN'] = NID_geoDAR.LONG_Geo\n",
    "\n",
    "print('Number of dams in database:', NID_geoDAR.shape)\n",
    "\n",
    "GRanD_join = NID_geoDAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91431dd",
   "metadata": {},
   "source": [
    "#### Filter based on storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fee971a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before storage filtering: (89846, 113)\n",
      "Size after storage filtering: (89846, 117)\n",
      "Number of sites in database: (1053, 117)\n",
      "Number of Reclamation dams in database: (201, 117)\n",
      "Number of USACE dams in database: (345, 117)\n",
      "Number of GRanD in database: (1897, 117)\n"
     ]
    }
   ],
   "source": [
    "#assign 0 to IsGRanD non-GRanD dams\n",
    "GRanD_join.IsGRanD = GRanD_join.IsGRanD.fillna(0)\n",
    "\n",
    "## Filter based on storage\n",
    "print('Size before storage filtering:',GRanD_join.shape)\n",
    "\n",
    "#Create column that takes the maximum of all of the storage values. This is NID_Storag in the NID table\n",
    "GRanD_join['MaxStor_m3'] = GRanD_join['NID_Storag'] * 1233.48 #Convert AF to m^3\n",
    "\n",
    "#Fill MaxStor nans with 0\n",
    "GRanD_join.MaxStor_m3 = GRanD_join.MaxStor_m3.fillna(0)\n",
    "\n",
    "\n",
    "#Set GRanD_join storage sources to initially be NID and reference year to initially be Year_Compl\n",
    "#The outcome of this is that for each storage value reported, you have a source and a year that storage value represents\n",
    "GRanD_join['StorSource'] = 'NID'\n",
    "GRanD_join['Stor_Refyr'] = GRanD_join.Year_Compl\n",
    "\n",
    "#Replace any with MaxStor == 0 with GRanD storage, site storage, then USBR/USACE storage, then removed dams\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = 'GRanD'\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.YEAR\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.GRanDCapm3 #GRanD\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.Year_First\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.CapOrigAF*1233.48 #site\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.yr_p\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_m3'] = GRanD_join.Capm3_p #Reclamation/USACE\n",
    "\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'StorSource'] = GRanD_join.Batch_for\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'Stor_Refyr'] = GRanD_join.yrc\n",
    "GRanD_join.loc[GRanD_join.MaxStor_m3 == 0, 'MaxStor_AF'] = GRanD_join.OrigCap_m3 #Removed dams\n",
    "\n",
    "\n",
    "#Any dams with max-storage = 0 that is not a site or federal is removed\n",
    "GRanD_join = GRanD_join.drop(index=GRanD_join.loc[GRanD_join.MaxStor_m3 == 0].loc[GRanD_join.IsSite==0 & (GRanD_join.IsUSBR == 0) & (GRanD_join.IsUSACE==0)].index)\n",
    "\n",
    "print('Size after storage filtering:',GRanD_join.shape)\n",
    "print('Number of sites in database:', GRanD_join.loc[GRanD_join.IsSite == 1].shape)\n",
    "print('Number of Reclamation dams in database:', GRanD_join.loc[GRanD_join.IsUSBR == 1].shape)\n",
    "print('Number of USACE dams in database:', GRanD_join.loc[GRanD_join.IsUSACE == 1].shape)\n",
    "print('Number of GRanD in database:', GRanD_join.loc[GRanD_join.IsGRanD == 1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cf9974",
   "metadata": {},
   "source": [
    "#### Assign ShortIDs to any dam that doesn't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad1a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign ShortID to non-sites\n",
    "# First, sort by ascending NID\n",
    "GRanD_join = GRanD_join.sort_values('NID',ascending = True)\n",
    "\n",
    "\n",
    "#starting ShortID should be the maximum of the site/GRanD ShortIDs plus 1,000 and rounded to the nearest thousandth\n",
    "startID = math.floor((GRanD_join.ShortID.max() + 1000)/1000)*1000\n",
    "ID = startID\n",
    "\n",
    "#Assign a ShortID to anything that doesn't have one yet.        \n",
    "for index, row in GRanD_join.iterrows():\n",
    "    if pd.isna(row['ShortID']):  # Check if ShortID is null\n",
    "        GRanD_join.loc[index, 'ShortID'] = ID\n",
    "        ID += 1  # Increment ID for next ShortID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d13adc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no duplicate ShortIDs\n"
     ]
    }
   ],
   "source": [
    "#check for duplicate ShortIDs. If there are duplicates, you have an error in your site input files. This is either\n",
    "#you assigned two dams with different NIDs the same ShortID or could have a typo in the NID field.\n",
    "test = GRanD_join.ShortID\n",
    "\n",
    "nodup = set(test)\n",
    "\n",
    "if len(nodup) != len(test):\n",
    "    print('There are duplicate ShortIDs!')\n",
    "\n",
    "    newlist = [] # empty list to hold unique elements from the list\n",
    "    duplist = [] # empty list to hold the duplicate elements from the list\n",
    "    for i in test:\n",
    "        if i not in newlist:\n",
    "            newlist.append(i)\n",
    "        else:\n",
    "            duplist.append(i) # this method catches the first duplicate entries, and appends them to the list\n",
    "            \n",
    "    # The next step is to print the duplicate entries, and the unique entries\n",
    "    print(\"List of duplicates\", duplist)\n",
    "else:\n",
    "    print('There are no duplicate ShortIDs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a6a107",
   "metadata": {},
   "source": [
    "#### Clean up columns and export as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5be214e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_33772\\2525771764.py:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[nan nan nan ... nan nan nan]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  GRanD_join.loc[GRanD_join.yrr.isnull()==True, 'yrr'] = GRanD_join.YrRemoved #site/USACE/USBR\n"
     ]
    }
   ],
   "source": [
    "#Combine columns where necessary and drop unwanted columns\n",
    "\n",
    "#Combine NID Dam height and removed dam heights into one field. Only put in removed dam where NID is null.\n",
    "GRanD_join['DamH_ft'] = GRanD_join.NID_Height\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull() == True),'DamH_ft'] = GRanD_join.DAmHft\n",
    "GRanD_join.loc[(GRanD_join.DamH_ft.isnull())==True,'DamH_ft'] = GRanD_join.DAM_HGT_M*3.28\n",
    "\n",
    "#Fill in Dam function from removed dam file everywhere PrimPurp is null.Then replace the null PrimaryPurp with Purp because some have null PrimaryPurp and non-null Purp (looking at you random person in SD)\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.DamFunctio\n",
    "GRanD_join.loc[(GRanD_join.PrimaryPurp.isnull() == True),'PrimaryPurp'] = GRanD_join.Purp\n",
    "\n",
    "#year completed\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull()==True, 'yrc'] = GRanD_join.Year_Compl #anywhere removed dam database is null, change to NID\n",
    "GRanD_join.loc[GRanD_join.IsSite == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.IsUSBR == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.IsUSACE == 1, 'yrc'] = GRanD_join.Year_Compl_site\n",
    "GRanD_join.loc[GRanD_join.yrc.isnull()==True, 'yrc'] = GRanD_join.YEAR #anything else still null fill with GRanD\n",
    "\n",
    "#year removed\n",
    "GRanD_join.loc[GRanD_join.yrr.isnull()==True, 'yrr'] = GRanD_join.YrRemoved #site/USACE/USBR\n",
    "GRanD_join.loc[GRanD_join.yrr.isnull()==True, 'yrr'] = GRanD_join.REM_YEAR #GRanD\n",
    "\n",
    "#dam name\n",
    "GRanD_join.loc[GRanD_join.Dam_Name=='','Dam_Name'] = np.nan #set values we made blank earlier back to nan\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Other_Dam #Replace missing NID names with NID other name first\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.Reservoir #then site\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.USBRname #then USBR\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.RES_NAME #then GRanD\n",
    "GRanD_join.loc[GRanD_join.Dam_Name.isnull()==True, 'Dam_Name'] = GRanD_join.DamName #then Removed file\n",
    "\n",
    "#Drop unwanted columns at this point.\n",
    "GRanD_join = GRanD_join.drop(['index','OtherStructureID','FederalID','Longitude','Latitude','River','Owner_Name',\n",
    "                             'Max_Storag','Normal_Sto','Lat','Long','Batch_for_join','ShortID_NID',\n",
    "                              'IsSite_NID','IsUSBR_NID','IsUSACE_NID','DAM_NAME','ALT_NAME',\n",
    "                              'ADMIN_UNIT','LONG_DD','LAT_DD','NewX_x','NewY_x','NIDnotes','ShortID_GRanD','HasNHD',\n",
    "                              'IsSite_GRanD','IsUSBR_GRanD','IsUSACE_GRanD','Field','LAT_GRAND','LONG_GRAND',\n",
    "                              'CitationID','CitationUR','DamAccessi','DamRiverNa','DamRiver_1','DamLocatio',\n",
    "                              'DamState_P','DamLatitud','DamLongitu','DamAccurac','DamOwner','Batch_for_rem',\n",
    "                              'LAT_Rem','LONG_Rem','NewX_y','NewY_y','LAT_Geo','LONG_Geo','NID_Height','DamH_m',\n",
    "                              'DAmHft','Purp','DamFunctio','SiteIsGRanD','GRanD_ID','Other_Dam','REM_YEAR','RES_NAME',\n",
    "                              'DamName','DAM_HGT_M','USACE_PROJECT_ID','YEAR'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8f13362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything to metric and drop the ft column\n",
    "GRanD_join['Dam_Len_m'] = GRanD_join.Dam_Length*0.3048\n",
    "GRanD_join['SA_m2'] = GRanD_join.Surface_Ar*4046.85642\n",
    "GRanD_join['DA_km2'] = GRanD_join.Drainage_A*2.58998811\n",
    "GRanD_join['MaxQ_m3s'] = GRanD_join.Max_Discha*0.028316847\n",
    "GRanD_join['DamH_m'] = GRanD_join.DamH_ft * 0.3048\n",
    "GRanD_join['NIDStor_m3'] = GRanD_join.NID_Storag*1233.48185532\n",
    "GRanD_join['GRanDCapm3'] = GRanD_join.CAP_MCM*(1e6)\n",
    "GRanD_join['elev_m'] = GRanD_join.elev_ft*0.3048\n",
    "\n",
    "\n",
    "GRanD_join = GRanD_join.drop(['Dam_Length','Surface_Ar','Drainage_A','Max_Discha','CapOrigAF','CapNewAF','CapAF_p',\n",
    "                             'DamNameAlt','NID_Storag','CAP_MCM','elev_ft','DamH_ft',\n",
    "                              ],axis=1)\n",
    "#Rename columns\n",
    "GRanD_join.rename(columns = {'OrigCap_m3':'OCapm3_Rem','LAT_JOIN':'LAT_FINAL','LONG_JOIN':'LONG_FINAL',\n",
    "                             'DA_km':'site_DA_km'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4889ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export final files as a csv\n",
    "GRanD.to_csv(os.path.join(out_folder,'GRanD.csv'))\n",
    "GRanD_join.to_csv(os.path.join(out_folder,'NID_GRanDjoin.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd9b8f8",
   "metadata": {},
   "source": [
    "# 2. Snap dams to NHDPlus Flowlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a70b2936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Tuesday, April 23, 2024 4:22:11 PM\",\"NID_filtered Successfully converted:  D:/ResSed/MediumResolution_DamLinkages/Manuscript/Outputs\\\\NID_filtered.shp\",\"Succeeded at Tuesday, April 23, 2024 4:22:35 PM (Elapsed Time: 23.71 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'D:\\\\ResSed\\\\MediumResolution_DamLinkages\\\\Manuscript\\\\Outputs'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Snap dams to NHDPlus HR Flowlines: must be done with arcPy. Ensure that all layers are in the same coordinate system (here we use NAD83)\n",
    "\n",
    "#First convert the csv to a shapefile\n",
    "XFieldName = 'LONG_FINAL'\n",
    "YFieldName = 'LAT_FINAL'\n",
    "newLayerName = \"NID_filtered\" #Name of your output shapefile\n",
    "\n",
    "spatialRef = arcpy.SpatialReference(4269) #spatial reference WKID for NAD83\n",
    "csvFilePath = os.path.join(out_folder,'NID_GRanDjoin.csv') #your filtered dam dataset csv\n",
    "\n",
    "arcpy.MakeXYEventLayer_management(csvFilePath, XFieldName, YFieldName, newLayerName, spatial_reference=spatialRef)\n",
    "arcpy.FeatureClassToShapefile_conversion(newLayerName, out_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f14eb51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class='gpresult'><h2>Messages</h2><div id='messages' data-messages='[\"Start Time: Tuesday, April 23, 2024 5:44:49 PM\",\"Succeeded at Tuesday, April 23, 2024 5:44:51 PM (Elapsed Time: 2.08 seconds)\"]' data-show='true'><div id = 'default' /></div></div>"
      ],
      "text/plain": [
       "<Result 'NIDFiltered_snap'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For NHDPlus\n",
    "NIDFiltered = os.path.join(out_folder,'NID_filtered.shp') #Link to your filtered NID shapefile\n",
    "\n",
    "NIDlyr = \"NIDlyr\" #create a layer file\n",
    "NHDlyr = \"NHDlyr\"\n",
    "arcpy.management.MakeFeatureLayer(NIDFiltered,NIDlyr) #convert the feature class to a layer to work from\n",
    "arcpy.management.MakeFeatureLayer(NHDFlowline,NHDlyr)\n",
    "\n",
    "#Run the near tool to get the new lat/long with near FType558\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE = 55800\")\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 0\") #select all non-sites, non-Reclamation, non-USACE, and non-GRanD to snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\")\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\")\n",
    "\n",
    "#Near 250 m to FType558 for everything that isn't a site\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Create a new field and populate it\n",
    "arcpy.management.AddField(NIDlyr,'NrX_Final',\"DOUBLE\")\n",
    "arcpy.management.AddField(NIDlyr,'NrY_Final',\"DOUBLE\")\n",
    "\n",
    "\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'CLEAR_SELECTION') #clear the flowline FType 558 Selection before transferring over values\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Near 500m to all flowlines for any that didn't snap and have MaxStor >= 4000 AF/5,0000,000 m^3\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And not GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"MaxStor_m3>=5e6\") #MaxStor >= 4000 AF\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"500 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Near 250m to all flowlines for any that didn't snap and still aren't a site\n",
    "arcpy.management.SelectLayerByAttribute(NHDlyr,'NEW_SELECTION',\"FCODE <> 56600\") #select NHD that isn't a coastline\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\") #Select any Dams that didn't snap\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsSite = 0\") #And any that aren't sites\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsGRanD = 0\") #And any that aren't GRanD\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSBR = 0\") #And not Reclamation\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'SUBSET_SELECTION',\"IsUSACE = 0\") #And not Army Corps\n",
    "\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "#Sites to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsSite = 1\") #Select all sites\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#GRanD to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsGRanD = 1\") #Select all GRanD\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#USBR to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSBR = 1\") #Select all USBR\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#USACE to nearest flowline using near 250m b/c should be in the correct place.\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION') #clear the NID selection\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"IsUSACE = 1\") #Select all USACE\n",
    "\n",
    "arcpy.analysis.Near(NIDlyr,NHDlyr,\"250 Meters\",\"LOCATION\",\"NO_ANGLE\",\"PLANAR\")\n",
    "\n",
    "#Transfer values over\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrX_Final\",\"!NEAR_X!\", \"PYTHON3\")\n",
    "arcpy.CalculateField_management(NIDlyr, \"NrY_Final\",\"!NEAR_Y!\", \"PYTHON3\")\n",
    "\n",
    "arcpy.management.DeleteField(NIDlyr,['NEAR_X','NEAR_Y','NEAR_DIST','NEAR_FID'])\n",
    "\n",
    "\n",
    "#Delete any features that did not snap.\n",
    "#First select non-snaps\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'CLEAR_SELECTION')\n",
    "arcpy.management.SelectLayerByAttribute(NIDlyr,'NEW_SELECTION',\"NrX_Final = -1\")\n",
    "\n",
    "#Delete the selected rows:\n",
    "arcpy.management.DeleteRows(NIDlyr)\n",
    "arcpy.GetCount_management(NIDlyr)\n",
    "\n",
    "#Display XY data using NearX and NearY to move the points onto the NHD Flowlines\n",
    "arcpy.management.MakeXYEventLayer(NIDlyr, \"NrX_Final\", \"NrY_Final\", 'NIDFiltered_snap', spatialRef)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04271eb",
   "metadata": {},
   "source": [
    "### Intersect with the flowline data to extract attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce578b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersect file with moved dams to the flowline file:\n",
    "arcpy.analysis.Intersect(['NIDFiltered_snap',NHDFlowline],os.path.join(out_folder,'NIDFiltered_snap.shp'),\"ALL\",None,\"INPUT\")\n",
    "\n",
    "#Export NID file\n",
    "dbf = Dbf5(os.path.join(out_folder,'NIDFiltered_snap.dbf'))\n",
    "df = dbf.to_dataframe()\n",
    "df.to_csv(os.path.join(out_folder,'NID_filtered_snapped.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7009ea",
   "metadata": {},
   "source": [
    "### Remove duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6a9d96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahurst\\AppData\\Local\\Temp\\1\\ipykernel_33772\\1637887619.py:4: DtypeWarning: Columns (25,188) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  NID = pd.read_csv(os.path.join(out_folder,'NID_filtered_snapped.csv')) #load data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size before duplicates removed: (75990, 192)\n"
     ]
    }
   ],
   "source": [
    "#Remove duplicates. When you run intersect in ArcGIS, any intersects that happen at the join of two lines gives two\n",
    "#results in the final table. We need to delete one of these.\n",
    "\n",
    "NID = pd.read_csv(os.path.join(out_folder,'NID_filtered_snapped.csv')) #load data\n",
    "NID_sort = NID.sort_values('ShortID', ascending = True) #sort by ascending ShortID\n",
    "\n",
    "print('Size before duplicates removed:',NID_sort.shape)\n",
    "\n",
    "#convert DF to dictionaries (struct-like); basically has format column->value\n",
    "NID_dict = NID_sort.to_dict(orient = 'records')\n",
    "dupl_ordered_dict = NID.to_dict(orient='records')\n",
    "\n",
    "#initialize empty list to store indices of non-duplicates\n",
    "dupind = []\n",
    "\n",
    "#Identify unique values and their counts\n",
    "shortID = [item['ShortID'] for item in dupl_ordered_dict]\n",
    "uniquevals,ia = np.unique(shortID, return_inverse = True)\n",
    "\n",
    "#Count the frequency of each index in ia\n",
    "bincounts = np.bincount(ia)\n",
    "\n",
    "#Zero out singles\n",
    "singles = uniquevals[bincounts <= 1]\n",
    "singleidx = [i for i, val in enumerate(shortID) if val in singles]\n",
    "for idx in singleidx:\n",
    "    shortID[idx] = 0\n",
    "    \n",
    "#Overwrite repeats\n",
    "repeats = uniquevals[bincounts > 1]\n",
    "shortID = np.array([np.where(repeats == val)[0][0] + 1 if val in repeats else val for val in shortID])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143faca",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_it = 0; #initialize a counter\n",
    "#Pull out FCODE and Hydrosequence fields to help with decision tree for removing duplicates\n",
    "FCODE = [item['FCODE'] for item in dupl_ordered_dict]\n",
    "Hydroseq = [item['Hydroseq'] for item in dupl_ordered_dict]\n",
    "\n",
    "\n",
    "for i in range(len(shortID)):\n",
    "    if shortID[i] == 0: #if it is not a duplicate, keep it\n",
    "        dupind.append(i)\n",
    "    elif skip_it>0: #or if we already dealt with it, update the counter so it gets skipped\n",
    "        skip_it -= 1\n",
    "        continue\n",
    "    else: #else the value is a duplicate\n",
    "        dup = [idx for idx, val in enumerate(shortID) if val == shortID[i]] #gives all indices of the duplicates\n",
    "        dup1 = dup[0]\n",
    "        dupskip = dup1 #keep track of what the first index was because we will change this\n",
    "        j = len(dup)\n",
    "        jskip = j #same for the length of the duplicates\n",
    "\n",
    "        Hydro = Hydroseq[dup1:dup[j-1]+1] #Pull out Hydrosequences as the duplicates\n",
    "\n",
    "        kept_indices = [i for i, x in enumerate(Hydro) if x not in [Hydroseq[i] for i in dupind]] #if a dam is already snapped to that flowline, remove the flowlines from the options to choose from\n",
    "\n",
    "        dup_test = [dup[i] for i in kept_indices]\n",
    "                \n",
    "        if len(dup_test) == 0: #all of the flowline options have already been used, in which case just keep them all. duplicate snaps are removed later.\n",
    "            dup_test = dup\n",
    "        \n",
    "        dup = dup_test\n",
    "        \n",
    "        dup1 = dup[0]\n",
    "        j = len(dup)\n",
    "        \n",
    "        Floc = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 55800] #pull out FCODE = 55800 for duplicates\n",
    "        coast = [index for index, value in enumerate(FCODE[dup1:dup[j-1]+1]) if value == 56600] #pull out any duplicates that are on a coast flowline\n",
    "        \n",
    "        \n",
    "    \n",
    "        if len(Floc) == 1: #If only one value is FType 558\n",
    "            dupind.append(dup[Floc[0]])\n",
    "        elif len(Floc) == j: #all of the values are 558, take smallest hydroseq (most downstream)\n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[minloc])\n",
    "        elif len(Floc) == 0: #none are FType 558         \n",
    "            Hydro = Hydroseq[dup1:dup[j-1]+1]\n",
    "           \n",
    "            if len(coast)>0:\n",
    "                Hydro = np.delete(Hydro,coast) #remove coastal values; any dam that snaps to a coast flowline has it's dam order messed up and can route along the coast\n",
    "                dup = np.delete(dup,coast)\n",
    "\n",
    "            minloc = np.argmin(Hydro) #currently taking minimum of the new hydro\n",
    "            dupind.append(dup[minloc])\n",
    "        else: #some other number of values is FType 558; still take the most downstream\n",
    "            Hydro = [Hydroseq[dup[index]] for index in Floc]\n",
    "    \n",
    "            if len(coast) > 0:\n",
    "                Hydro = np.delete(Hydro,coast)\n",
    "                dup = np.delete(dup,coast)\n",
    "                \n",
    "            minloc = np.argmin(Hydro)\n",
    "            dupind.append(dup[Floc[minloc]])\n",
    "            \n",
    "        if dupskip == i: #if the first index was the current index\n",
    "            skip_it = jskip-1 #skip the next j-1 indices\n",
    "        else:\n",
    "            skip_it = 0\n",
    "\n",
    "dupltable = pd.DataFrame.from_dict(dupl_ordered_dict)\n",
    "noduplicates = dupltable.loc[dupind]\n",
    "\n",
    "noduplicates.to_csv(os.path.join(out_folder,'NID_filtered_snapped_nodupl.csv'),index = False)\n",
    "\n",
    "\n",
    "print('Size after removing duplicates:',noduplicates.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f69d93",
   "metadata": {},
   "source": [
    "# 3. Join D50 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NID_D50 = pd.merge(noduplicates, D50, on='COMID', how='left')\n",
    "NID_D50 = NID_D50.drop(['Shape_Leng','OID_','StreamOrde_y','TotDASqKM_y'],axis=1)\n",
    "NID_D50.rename(columns = {\"TotDASqKM_x\":\"TotDASqKM\",\"StreamOrde_x\":\"StreamOrder\"},inplace=True)\n",
    "\n",
    "print('Size after joining D50:',NID_D50.shape)\n",
    "\n",
    "NID_D50.to_csv(os.path.join(out_folder,'NID_filtered_snapped_nodupl_D50.csv'),index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
